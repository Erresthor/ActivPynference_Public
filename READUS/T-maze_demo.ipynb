{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> active-pynference </b> : T-maze demo\n",
    "\n",
    "Hello you ! This is a quick demo of the <b>active-pynference</b> / <b>actynf</b> package to simulate MDPs using Sophisticated Inference ! \n",
    "Buckle up buckaroo !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducing the task\n",
    "\n",
    "To demonstrate the ability of the Sophisticated Inference algorithm to predict various behaviours in an explore/exploit environment, we will introduce a environment well known within the Active Inference community : the T-maze environment.\n",
    "\n",
    "Let's picture a mouse in a simple maze :\n",
    "\n",
    "![starting_situation.png](local_resources/tmaze/starting_situation.png)\n",
    "\n",
    "This maze consists in two objects (here : a cheese and a mousetrap) that are either on the left or the right branch. The initial position of those objects is determined by the experimenter (you !). Formally, we can note $s_1$ the state relative to the position of the cheese (either left or right). The initial position of the cheese is determined by the following probability distribution : $D_1 = [p_{init},1-p_{init}]$ and will not change during a trial.\n",
    "\n",
    "The mouse may be in four different places : on its starting position (0),on the bottom of the maze (1), on the left(2) or on the right (3). Let's call this second state $s_2$. Initially, the value of $s_2$ is always 0.\n",
    "\n",
    "The mouse always wants to get to observe cheese as fast as possible and wants to stay away from observing the mousetrap. How much the mouse is looking after the reward and how much it fears the trap is fixed by the experimenter through preference parameters called *reward seeking* (rs)  and *loss aversion* (la).\n",
    "\n",
    "**Note :** To discourage greedy mouses, once it has picked either left or right, it is stuck for the remainder of the trial. Therefore, the mouse only has one chance at guessing where the cheese is.\n",
    "\n",
    "This wouldn't be a very interesting setup if we didn't add another dimension to the task : the clue. If the mouse chooses to get to the bottom of the maze, it will receive a clue. Although this clue has no extrinsic reward, it may (or may not) contain some relevant information regarding the position of the cheese. For example, if the clue is good, it will indicate reliably a certain value if the cheese is left and another if the cheese is right.If its not, the observation it provides the mouse will have no correlation with the position of the cheese whatsoever, making it useless. We can picture those clue observation values as arrows pointing towards the right or the left : \n",
    "\n",
    "Reliable clue                           | Unreliable clue                     \n",
    ":--------------------------------------:|:------------------------------------:\n",
    "![](local_resources/tmaze/goodclue.gif) |![](local_resources/tmaze/badclue.gif)\n",
    "\n",
    "The point of this task is to explore how various parameters such as the mouse initial perception of the task or the environmental dynamcis may affect its behaviour : *Should I get the clue,resolving uncertainty but differing my reward ? Should I risk going for the cheese even if I'm not sure about its position ? How good is the clue ?*\n",
    "\n",
    "On the next part of this tutorial , we'll see how to simulate various mouse behaviours using *active_pynference*.\n",
    "\n",
    "## 2 . Using the package\n",
    "\n",
    "### a. Install the package & import the needed packages \n",
    "<sup><sub><b> active-pynference </b> requires Python 3.x. and has been tested for Python 3.11 + but probably works well enough with slightly older versions.</sub></sup>\n",
    "\n",
    "You can install the package by running :\n",
    "\n",
    "```\n",
    "    pip install active-pynference\n",
    "```\n",
    "\n",
    "You can find more complete documentation regarding the package installation in the installation_instructions.ipynb file.\n",
    "\n",
    "Now that the package is successfully installed, let's explore what we can do with it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported active-pynference - version 0.1.2\n"
     ]
    }
   ],
   "source": [
    "# First, let's import stuff !\n",
    "# Python \"classics\": \n",
    "import numpy as np\n",
    "\n",
    "# Active Inference based packages :\n",
    "import actynf # import active-pynference package\n",
    "print(\"Imported active-pynference - version \" + actynf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Set up the environment and the mouse model\n",
    "\n",
    "The active-pynference package relies on a generic component to build both subject environments and models. This generic component is the <i> layer </i>.\n",
    "Let's import it using :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actynf.layer.model_layer import mdp_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *actynf*, the **mdp_layer** is a generic Python class that can be used to compute observations from states and actions (a generative process) as well as infer states and actions from observations and model variables (a generative model). All the user has to do to differentiate between those behaviours is to specify it in the constructor.\n",
    "\n",
    "Let's build the environment for our T-maze example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-maze gen. process set-up ...  Done.\n",
      "LAYER T-maze_environment : \n",
      " -------------------------------------\n",
      "LAYER DIMENSION REPORT (T-maze_environment): \n",
      "\n",
      "Observation modalities : 3\n",
      "    Modality 0 : 3 outcomes.\n",
      "    Modality 1 : 3 outcomes.\n",
      "    Modality 2 : 4 outcomes.\n",
      "Hidden states factors : 2\n",
      "    Model factor 0 : 2 possible states. \n",
      "    Model factor 1 : 4 possible states. \n",
      "Number of potential actions : 4\n",
      "    Factor 0 : 1 possible transitions. \n",
      "    Factor 1 : 4 possible transitions. \n",
      "-------------------------------------\n",
      "\n",
      "##################################################\n",
      "Layer weights :\n",
      "   Matrix a :\n",
      "     Modality 0 :\n",
      "[[[1. 0. 1. 1.]\n",
      "  [1. 0. 1. 1.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]]]\n",
      "     Modality 1 :\n",
      "[[[1. 1. 0. 0.]\n",
      "  [1. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]]]\n",
      "     Modality 2 :\n",
      "[[[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]]]\n",
      "   Matrix b :\n",
      "     Factor : 0  --- Transition 0 :\n",
      "[[1 0]\n",
      " [0 1]]\n",
      "     Factor : 1  --- Transition 0 :\n",
      "[[1. 1. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "     Factor : 1  --- Transition 1 :\n",
      "[[0. 1. 1. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "     Factor : 1  --- Transition 2 :\n",
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "     Factor : 1  --- Transition 3 :\n",
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]]\n",
      "   Matrix c :\n",
      "Modality 0 :\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Modality 1 :\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Modality 2 :\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "   Matrix d :\n",
      "     Factor 0 :\n",
      "[0.5 0.5]\n",
      "     Factor 1 :\n",
      "[1 0 0 0]\n",
      "   Matrix e :\n",
      "[1. 1. 1. 1.]\n",
      "   Allowable actions u :\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 3]]\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "def build_tmaze_process(pinit,pHA,pWin):\n",
    "    \"\"\"\n",
    "    pinit : prob of reward initial position being left / right\n",
    "    pHA : probability of clue giving the correct index position\n",
    "    pWin : probability of winning if we are in the correct position\n",
    "\n",
    "    This function returns a mdp_layer representing the t-maze environment.\n",
    "    \"\"\"\n",
    "    print(\"T-maze gen. process set-up ...  \",end='')\n",
    "\n",
    "    T = 3  # The trials are made of 3 timesteps (starting step + 2 others)\n",
    "\n",
    "    # Initial situation\n",
    "    d = [np.array([pinit,1-pinit])    ,np.array([1,0,0,0])]\n",
    "    #  on which side is the cheese | where is the mouse \n",
    "    Ns = [arr.shape[0] for arr in d] # Number of states\n",
    "    \n",
    "    # Transition matrixes between hidden states=\n",
    "    # a. Transition between cheese states --> the cheese doesn't move during the trial, and the mouse can't make it move :\n",
    "    B_context_states = np.array([[[1],[0]],\n",
    "                                 [[0],[1]]])\n",
    "    # b. Transition between mouse position states --> 4 actions possible for the mouse\n",
    "    B_behav_states = np.zeros((Ns[1],Ns[1],Ns[1]))\n",
    "\n",
    "    # - 0 --> Move to start from any state\n",
    "    B_behav_states[0,:,0] = 1          \n",
    "    # - 1 --> Move to clue from start, else go to start\n",
    "    B_behav_states[:,:,1] = np.array([[0,1,1,1],\n",
    "                                      [1,0,0,0],\n",
    "                                      [0,0,0,0],\n",
    "                                      [0,0,0,0]])\n",
    "    # - 2 --> Move to choose left from start or hint, else go to start\n",
    "    B_behav_states[:,:,2] = np.array([[0,0,1,1],\n",
    "                                      [0,0,0,0],\n",
    "                                      [1,1,0,0],\n",
    "                                      [0,0,0,0]])  \n",
    "    \n",
    "    # - 3 --> Move to choose right from start or hint, else go to start\n",
    "    B_behav_states[:,:,3] = np.array([[0,0,1,1],\n",
    "                                      [0,0,0,0],\n",
    "                                      [0,0,0,0],\n",
    "                                      [1,1,0,0]])  \n",
    "    b = [B_context_states, B_behav_states]\n",
    "    # Note : as you can see, the mouse can't go to right then left or left then right : every trial, it has to make a decision between the two.\n",
    "\n",
    "    # Active Inference also revolves around a state-observation correspondance that we describe here :\n",
    "    \n",
    "\n",
    "    # 1. Mapping from states to observed hints, depending on cheese & mouse states\n",
    "    #\n",
    "    # [ .  . ]  No hint\n",
    "    # [ .  . ]  Left Hint            Rows = observations\n",
    "    # [ .  . ]  Right Hint\n",
    "    # Left Right\n",
    "    # Columns = cheese state\n",
    "    A_obs_hints = np.zeros((3,Ns[0],Ns[1]))\n",
    "    A_obs_hints[0,:,:] = 1\n",
    "    A_obs_hints[:,:,1] = np.array([[0,0],\n",
    "                             [pHA, 1-pHA],\n",
    "                             [1-pHA,pHA]]) # We only get the clue if the mouse moves to state 1\n",
    "    \n",
    "    # 2. Mapping from states to outcome (win / loss / null), depending on cheese & mouse states\n",
    "    #\n",
    "    # [ .  . ]  Null\n",
    "    # [ .  . ]  Win           Rows = observations\n",
    "    # [ .  . ]  Loss\n",
    "    # Left Right\n",
    "    # Columns = cheese state\n",
    "    A_obs_outcome = np.zeros((3,Ns[0],Ns[1]))\n",
    "    A_obs_outcome[0,:,:2] = 1\n",
    "    A_obs_outcome[:,:,2] = np.array([[0,0],   # If we choose left, what is the probability of achieving win / loss \n",
    "                             [pWin, 1-pWin],\n",
    "                             [1-pWin,pWin]]) # Choice gives an observable outcome\n",
    "                   # If true = left, right\n",
    "    A_obs_outcome[:,:,3] = np.array([[0,0],     # If we choose right, what is the probability of achieving win / loss \n",
    "                                     [1-pWin, pWin],\n",
    "                                     [pWin,1-pWin]]) # Choice gives an observable outcome\n",
    "                  # If true = left, right\n",
    "    \n",
    "    # 3. Mapping from mouse position states to observed mouse position\n",
    "    #\n",
    "    # [ .  .  .  .] start\n",
    "    # [ .  .  .  .] hint\n",
    "    # [ .  .  .  .] choose left         Row = Behaviour state\n",
    "    # [ .  .  .  .] choose right\n",
    "    #  s   h  l  r\n",
    "    #\n",
    "    # 3rd dimension = observed behaviour\n",
    "    # The 2nd dimension maps the dependance on cheese state (unvariant)\n",
    "    A_obs_behaviour = np.zeros((Ns[1],Ns[0],Ns[1]))\n",
    "    for i in range (Ns[1]) :\n",
    "        A_obs_behaviour[i,:,i] = np.array([1,1])\n",
    "    a = [A_obs_hints,A_obs_outcome,A_obs_behaviour]\n",
    "\n",
    "    No = [ai.shape[0] for ai in a] # Number of outcomes\n",
    "\n",
    "    # Finally, we set up the preferences of the environment (this is an environment, thus this is empty) ...\n",
    "    c = [np.zeros((No[0],T)),np.zeros((No[1],T)),np.zeros((No[2],T))]\n",
    "    # ... as well as the allowable transitions the mouse can choose :\n",
    "    u = np.array([[0,0],[0,1],[0,2],[0,3]]).astype(int)\n",
    "    \n",
    "    # Habits\n",
    "    e = np.ones((u.shape[0],))\n",
    "\n",
    "    # The environment has been well defined and we may now build a mdp_layer using the following constructor : \n",
    "    layer = mdp_layer(\"T-maze_environment\",\"process\",a,b,c,d,e,u,T)\n",
    "    #     mdp_layer(name of the layer,process or model, a,b,c,d,e,u,T)\n",
    "    print(\"Done.\")\n",
    "    return layer\n",
    "\n",
    "# We can test that the layer was well defined by instantiating and building it :\n",
    "tmaze_environment = build_tmaze_process(0.5,1.0,1.0)\n",
    "print(tmaze_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can see a general overview of the layer we just defined. One advantage of using the same object for processes and models is that we can easily use the same object for both purposes if needed. Let's now define the model our mouse is going to entertain (don't worry, it'll be much quicker :) ) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-maze gen. model set-up ...  Done.\n",
      "LAYER mouse_model : \n",
      " -------------------------------------\n",
      "LAYER DIMENSION REPORT (mouse_model): \n",
      "\n",
      "Observation modalities : 3\n",
      "    Modality 0 : 3 outcomes.\n",
      "    Modality 1 : 3 outcomes.\n",
      "    Modality 2 : 4 outcomes.\n",
      "Hidden states factors : 2\n",
      "    Model factor 0 : 2 possible states. \n",
      "    Model factor 1 : 4 possible states. \n",
      "Number of potential actions : 4\n",
      "    Factor 0 : 1 possible transitions. \n",
      "    Factor 1 : 4 possible transitions. \n",
      "-------------------------------------\n",
      "\n",
      "##################################################\n",
      "Layer weights :\n",
      "   Matrix a :\n",
      "     Modality 0 :\n",
      "[[[200.     0.   200.   200.  ]\n",
      "  [200.     0.   200.   200.  ]]\n",
      "\n",
      " [[  0.     0.25   0.     0.  ]\n",
      "  [  0.     0.25   0.     0.  ]]\n",
      "\n",
      " [[  0.     0.25   0.     0.  ]\n",
      "  [  0.     0.25   0.     0.  ]]]\n",
      "     Modality 1 :\n",
      "[[[200. 200.   0.   0.]\n",
      "  [200. 200.   0.   0.]]\n",
      "\n",
      " [[  0.   0. 200.   0.]\n",
      "  [  0.   0.   0. 200.]]\n",
      "\n",
      " [[  0.   0.   0. 200.]\n",
      "  [  0.   0. 200.   0.]]]\n",
      "     Modality 2 :\n",
      "[[[200.   0.   0.   0.]\n",
      "  [200.   0.   0.   0.]]\n",
      "\n",
      " [[  0. 200.   0.   0.]\n",
      "  [  0. 200.   0.   0.]]\n",
      "\n",
      " [[  0.   0. 200.   0.]\n",
      "  [  0.   0. 200.   0.]]\n",
      "\n",
      " [[  0.   0.   0. 200.]\n",
      "  [  0.   0.   0. 200.]]]\n",
      "   Matrix b :\n",
      "     Factor : 0  --- Transition 0 :\n",
      "[[200   0]\n",
      " [  0 200]]\n",
      "     Factor : 1  --- Transition 0 :\n",
      "[[200. 200. 200. 200.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]]\n",
      "     Factor : 1  --- Transition 1 :\n",
      "[[  0. 200. 200. 200.]\n",
      " [200.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]]\n",
      "     Factor : 1  --- Transition 2 :\n",
      "[[  0.   0. 200. 200.]\n",
      " [  0.   0.   0.   0.]\n",
      " [200. 200.   0.   0.]\n",
      " [  0.   0.   0.   0.]]\n",
      "     Factor : 1  --- Transition 3 :\n",
      "[[  0.   0. 200. 200.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [200. 200.   0.   0.]]\n",
      "   Matrix c :\n",
      "Modality 0 :\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Modality 1 :\n",
      "[[ 0.   0.   0. ]\n",
      " [ 0.   3.   1.5]\n",
      " [ 0.  -2.  -2. ]]\n",
      "Modality 2 :\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "   Matrix d :\n",
      "     Factor 0 :\n",
      "[0.25 0.25]\n",
      "     Factor 1 :\n",
      "[1 0 0 0]\n",
      "   Matrix e :\n",
      "[1. 1. 1. 1.]\n",
      "   Allowable actions u :\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 3]]\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_mouse_model(true_process_layer,la,rs,T_horizon,initial_clue_confidence = 0.1):\n",
    "    \"\"\"\n",
    "    true_process_layer : the mdp_layer object where the tmaze environment has been defined\n",
    "    la : how much the mouse is afraid of adverse outcomes (>0)\n",
    "    rs : how much the mouse wants to observe cheese (>0)\n",
    "    T_horizon : how much into the future the mouse will plan before picking its next action\n",
    "    initial_clue_confidence : how much the mouse knows about the clue reliability\n",
    "    \"\"\"\n",
    "    print(\"T-maze gen. model set-up ...  \",end='')\n",
    "    T = 3\n",
    "\n",
    "    #  The mouse knows where it stands in the maze initially, but it doesn't know where the cheese will spawn : this is something that\n",
    "    # it will need to learn !\n",
    "    d = [np.array([0.25,0.25]),np.array([1,0,0,0])]\n",
    "\n",
    "    \n",
    "    # Transition matrixes between hidden states ( = control states)\n",
    "    b=[]\n",
    "    for b_fac_proc in (true_process_layer.b):\n",
    "        b.append(np.copy(b_fac_proc)*200)\n",
    "    # The mouse knows how its action will affect the general situation. The mouse does not need\n",
    "    # to learn that element . Be aware that too much uncertainty in some situations may prove hard to resolve for our\n",
    "    # artifical subjects.\n",
    "\n",
    "\n",
    "    a = []\n",
    "    for a_mod_proc in (true_process_layer.a):\n",
    "        a.append(np.copy(a_mod_proc)*200)\n",
    "    a[0][:,:,1] = initial_clue_confidence*np.array([[0,0],\n",
    "                                                    [0.25,0.25],\n",
    "                                                    [0.25,0.25]])  \n",
    "    # The mouse already knows how the cheese position and its own position in the \n",
    "    # maze relates relates to its probability to observe cheese. It also knows where\n",
    "    # it is in the maze at all times. It knows this because it knows where it isn't ;)\n",
    "    # However, the mouse still has to learn the reliability of the clue.\n",
    "\n",
    "\n",
    "    # Finally, the preferences of the mouse are governed by the experimenter through the rs/la weights.\n",
    "    No = [ai.shape[0] for ai in a]\n",
    "\n",
    "    C_hints = np.zeros((No[0],T))\n",
    "    C_win_loss = np.zeros((No[1],T))\n",
    "    C_win_loss = np.array([[0,0,0],     #null\n",
    "                           [0,rs,rs/2.0],  #win : as you can see, the mouse would much rather find the cheese at timestep 2 rather than 3. Feel free to play with this factor.\n",
    "                           [0,-la,-la]]) #loss\n",
    "    C_observed_behaviour = np.zeros((No[2],T))\n",
    "    c = [C_hints,C_win_loss,C_observed_behaviour]\n",
    "    # The mouse has no preference towards seeing a clue or being in a given position. However, it does have a preference regarding\n",
    "    # the outcome of the trial (i.e. seeing the cheese or the mousetrap)\n",
    "    \n",
    "    # The allowable actions have been defined earlier\n",
    "    u = true_process_layer.U\n",
    "    # u = np.array([[0,0],[0,1],[0,2],[0,3]]).astype(int)\n",
    "    \n",
    "    # Habits\n",
    "    e = np.ones((u.shape[0],))\n",
    "\n",
    "    layer = mdp_layer(\"mouse_model\",\"model\",a,b,c,d,e,u,T,T_horiz=T_horizon)\n",
    "    # This time, we define our layer as a \"model\" \n",
    "\n",
    "    # Here, we give a few hyperparameters guiding the beahviour of our agent :\n",
    "    layer.hyperparams.alpha = 32 # action precision : \n",
    "        # for high values the mouse will always perform the action it perceives as optimal, with very little exploration \n",
    "        # towards actions with similar but slightly lower interest\n",
    "\n",
    "    layer.learn_options.eta = 1 # learning rate (shared by all channels : a,b,c,d,e)\n",
    "    layer.learn_options.learn_a = True  # The agent learns the reliability of the clue\n",
    "    layer.learn_options.learn_b = False # The agent does not learn transitions\n",
    "    layer.learn_options.learn_d = True  # The agent has to learn the initial position of the cheese\n",
    "    layer.learn_options.backwards_pass = True  # When learning, the agent will perform a backward pass, using its perception of \n",
    "                                               # states in later trials (e.g. I saw that the cheese was on the right at t=3)\n",
    "                                               # as well as what actions it performed (e.g. and I know that the cheese position has\n",
    "                                               # not changed between timesteps) to learn more reliable weights (therefore if my clue was\n",
    "                                               # a right arrow at time = 2, I should memorize that cheese on the right may correlate with\n",
    "                                               # right arrow in general)\n",
    "    print(\"Done.\")\n",
    "    return layer\n",
    "\n",
    "mouse_model = build_mouse_model(tmaze_environment,2,3,3,1.0)\n",
    "print(mouse_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our environment (generative process) and the model our mouse will entertain (generative model), we need to describe how the two will interact to form a system. \n",
    "\n",
    "To create interactions between layers, we need to establish *links* between some of their inputs and outputs :\n",
    "- The environment outputs (outcomes) are forwarded to the mouse sensory states (observations)\n",
    "- The mouse actions (active states) lead to a changes in the environment\n",
    "\n",
    "The resulting interconnected system will then form a dedicated *actynf* object called a *network*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established layerLink between T-maze_environment and mouse_model.\n"
     ]
    }
   ],
   "source": [
    "from actynf.layer.layer_link import establish_layerLink # the function we use to establish links between layers\n",
    "\n",
    "#1. Create a link from observations generated by the environment to the mouse sensory states :\n",
    "link_obs = establish_layerLink(tmaze_environment,mouse_model,[\"o\",\"o\"])\n",
    "link_act = establish_layerLink(mouse_model,tmaze_environment,[\"u\",\"u\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behavioural_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
