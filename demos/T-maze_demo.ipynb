{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> active-pynference </b> : T-maze demo\n",
    "\n",
    "Hello you ! This is a quick demo of the <b>active-pynference</b> / <b>actynf</b> package to simulate MDPs using Sophisticated Inference ! \n",
    "Buckle up buckaroo !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducing the task\n",
    "\n",
    "To demonstrate the ability of the Sophisticated Inference algorithm to predict various behaviours in an explore/exploit environments, we will focus on a environment well known within the Active Inference community : the T-maze.\n",
    "\n",
    "Let's picture a mouse in a simple maze :\n",
    "\n",
    "![starting_situation.png](local_resources/tmaze/starting_situation.png)\n",
    "\n",
    "This maze consists in two objects (here : a cheese and a mousetrap) that are either on the left or the right branch. The initial position of those objects is determined by the experimenter (you !). Formally, we can note $s_1$ the state relative to the position of the cheese (either left or right). The initial position of the cheese is determined by the following probability distribution : $D_1 = [p_{init},1-p_{init}]$ and will not change during a trial.\n",
    "\n",
    "The mouse may be in four different places : on its starting position (0),on the bottom of the maze (1), on the left(2) or on the right (3). Let's call this second state $s_2$. Initially, the value of $s_2$ is always 0.\n",
    "\n",
    "The mouse always wants to get to observe cheese as fast as possible and wants to stay away from observing the mousetrap. How much the mouse is looking after the reward and how much it fears the trap is fixed by the experimenter through preference parameters called *reward seeking* (rs)  and *loss aversion* (la).\n",
    "\n",
    "**Note :** To discourage greedy mouses, once it has picked either left or right, it is stuck for the remainder of the trial. Therefore, the mouse only has one chance at guessing where the cheese is.\n",
    "\n",
    "This wouldn't be a very interesting setup if we didn't add another dimension to the task : the clue. If the mouse chooses to get to the bottom of the maze, it will receive a clue. Although this clue has no extrinsic reward, it may (or may not) contain some relevant information regarding the position of the cheese. For example, if the clue is good, it will indicate reliably a certain value if the cheese is left and another if the cheese is right.If its not, the observation it provides the mouse will have no correlation with the position of the cheese whatsoever, making it useless. We can picture those clue observation values as arrows pointing towards the right or the left : \n",
    "\n",
    "Reliable clue                           | Unreliable clue                     \n",
    ":--------------------------------------:|:------------------------------------:\n",
    "![](local_resources/tmaze/goodclue.gif) |![](local_resources/tmaze/badclue.gif)\n",
    "\n",
    "The point of this task is to explore how various parameters such as the mouse initial perception of the task or the environmental dynamics may affect its behaviour : *Should I get the clue,resolving uncertainty but differing my reward ? Should I risk going for the cheese even if I'm not sure about its position ? How good is the clue ?*\n",
    "\n",
    "Casper Hesp and colleagues made this nice rendition of the paradigm (replace food with cheese and shock with mousetrap):\n",
    "\n",
    "![](local_resources/tmaze/tmaze_article_figure_hesp.png)\n",
    "\n",
    "<sub><sup>(**Source :** Hesp, Casper & Smith, Ryan & Parr, Thomas & Allen, Micah & Friston, Karl & Ramstead, Maxwell. (2019). Deeply Felt Affect: The Emergence of Valence in Deep Active Inference. 10.31234/osf.io/62pfd.)</sub></sup>\n",
    "\n",
    "On the next part of this tutorial , we'll see how to simulate various mouse behaviours using *active_pynference*.\n",
    "\n",
    "## 2 . Using the package\n",
    "\n",
    "### a. Install the package & import the needed packages \n",
    "<sup><sub><b> active-pynference </b> requires Python 3.x. and has been tested for Python 3.11 + but probably works well enough with slightly older versions.</sub></sup>\n",
    "\n",
    "You can install the package by running :\n",
    "\n",
    "```\n",
    "    pip install active-pynference\n",
    "```\n",
    "\n",
    "You can find more complete documentation regarding the package installation in the installation_instructions.ipynb file.\n",
    "\n",
    "Now that the package is successfully installed, let's explore what we can do with it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported active-pynference - version 0.1.31\n"
     ]
    }
   ],
   "source": [
    "# First, let's import stuff !\n",
    "# Python \"classics\": \n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Active Inference based packages :\n",
    "import actynf # import active-pynference package\n",
    "print(\"Imported active-pynference - version \" + actynf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Set up the environment and the mouse model\n",
    "\n",
    "The active-pynference package relies on a generic component to build both subject environments and models. This generic component is the <i> layer </i>.\n",
    "Let's import it using :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actynf.layer.model_layer import mdp_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *actynf*, the **mdp_layer** is a generic Python class that can be used to compute observations from states and actions (a generative process) as well as infer states and actions from observations and model variables (a generative model). All the user has to do to differentiate between those behaviours is to specify it in the constructor.\n",
    "\n",
    "Let's build the environment for our T-maze example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-maze gen. process set-up ...  Done.\n",
      "LAYER T-maze_environment : \n",
      " -------------------------------------\n",
      "LAYER DIMENSION REPORT (T-maze_environment): \n",
      "\n",
      "Observation modalities : 3\n",
      "    Modality 0 : 3 outcomes.\n",
      "    Modality 1 : 3 outcomes.\n",
      "    Modality 2 : 4 outcomes.\n",
      "Hidden states factors : 2\n",
      "    Model factor 0 : 2 possible states. \n",
      "    Model factor 1 : 4 possible states. \n",
      "Number of potential actions : 4\n",
      "    Factor 0 : 1 possible transitions. \n",
      "    Factor 1 : 4 possible transitions. \n",
      "-------------------------------------\n",
      "\n",
      "##################################################\n",
      "Layer weights :\n",
      "   Matrix a :\n",
      "     Modality 0 :\n",
      "[[[1. 0. 1. 1.]\n",
      "  [1. 0. 1. 1.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]]]\n",
      "     Modality 1 :\n",
      "[[[1. 1. 0. 0.]\n",
      "  [1. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]]]\n",
      "     Modality 2 :\n",
      "[[[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]]]\n",
      "   Matrix b :\n",
      "     Factor : 0  --- Transition 0 :\n",
      "[[1 0]\n",
      " [0 1]]\n",
      "     Factor : 1  --- Transition 0 :\n",
      "[[1. 1. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "     Factor : 1  --- Transition 1 :\n",
      "[[0. 1. 1. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "     Factor : 1  --- Transition 2 :\n",
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "     Factor : 1  --- Transition 3 :\n",
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]]\n",
      "   Matrix c :\n",
      "Modality 0 :\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Modality 1 :\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Modality 2 :\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "   Matrix d :\n",
      "     Factor 0 :\n",
      "[0.5 0.5]\n",
      "     Factor 1 :\n",
      "[1 0 0 0]\n",
      "   Matrix e :\n",
      "[1. 1. 1. 1.]\n",
      "   Allowable actions u :\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 3]]\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "def build_tmaze_process(pinit,pHA,pWin):\n",
    "    \"\"\"\n",
    "    pinit : prob of reward initial position being left / right\n",
    "    pHA : probability of clue giving the correct index position\n",
    "    pWin : probability of winning if we are in the correct position\n",
    "\n",
    "    This function returns a mdp_layer representing the t-maze environment.\n",
    "    \"\"\"\n",
    "    print(\"T-maze gen. process set-up ...  \",end='')\n",
    "\n",
    "    T = 3  # The trials are made of 3 timesteps (starting step + 2 others)\n",
    "\n",
    "    # Initial situation\n",
    "    d = [np.array([pinit,1-pinit])    ,np.array([1,0,0,0])]\n",
    "    #  on which side is the cheese | where is the mouse \n",
    "    Ns = [arr.shape[0] for arr in d] # Number of states\n",
    "    \n",
    "    # Transition matrixes between hidden states=\n",
    "    # a. Transition between cheese states --> the cheese doesn't move during the trial, and the mouse can't make it move :\n",
    "    B_context_states = np.array([[[1],[0]],\n",
    "                                 [[0],[1]]])\n",
    "    # b. Transition between mouse position states --> 4 actions possible for the mouse\n",
    "    B_behav_states = np.zeros((Ns[1],Ns[1],Ns[1]))\n",
    "\n",
    "    # - 0 --> Move to start from any state\n",
    "    B_behav_states[0,:,0] = 1          \n",
    "    # - 1 --> Move to clue from start, else go to start\n",
    "    B_behav_states[:,:,1] = np.array([[0,1,1,1],\n",
    "                                      [1,0,0,0],\n",
    "                                      [0,0,0,0],\n",
    "                                      [0,0,0,0]])\n",
    "    # - 2 --> Move to choose left from start or hint, else go to start\n",
    "    B_behav_states[:,:,2] = np.array([[0,0,1,1],\n",
    "                                      [0,0,0,0],\n",
    "                                      [1,1,0,0],\n",
    "                                      [0,0,0,0]])  \n",
    "    \n",
    "    # - 3 --> Move to choose right from start or hint, else go to start\n",
    "    B_behav_states[:,:,3] = np.array([[0,0,1,1],\n",
    "                                      [0,0,0,0],\n",
    "                                      [0,0,0,0],\n",
    "                                      [1,1,0,0]])  \n",
    "    b = [B_context_states, B_behav_states]\n",
    "    # Note : as you can see, the mouse can't go to right then left or left then right : every trial, it has to make a decision between the two.\n",
    "\n",
    "    # Active Inference also revolves around a state-observation correspondance that we describe here :\n",
    "    \n",
    "\n",
    "    # 1. Mapping from states to observed hints, depending on cheese & mouse states\n",
    "    #\n",
    "    # [ .  . ]  No hint\n",
    "    # [ .  . ]  Left Hint            Rows = observations\n",
    "    # [ .  . ]  Right Hint\n",
    "    # Left Right\n",
    "    # Columns = cheese state\n",
    "    A_obs_hints = np.zeros((3,Ns[0],Ns[1]))\n",
    "    A_obs_hints[0,:,:] = 1\n",
    "    A_obs_hints[:,:,1] = np.array([[0,0],\n",
    "                             [pHA, 1-pHA],\n",
    "                             [1-pHA,pHA]]) # We only get the clue if the mouse moves to state 1\n",
    "    \n",
    "    # 2. Mapping from states to outcome (win / loss / null), depending on cheese & mouse states\n",
    "    #\n",
    "    # [ .  . ]  Null\n",
    "    # [ .  . ]  Win           Rows = observations\n",
    "    # [ .  . ]  Loss\n",
    "    # Left Right\n",
    "    # Columns = cheese state\n",
    "    A_obs_outcome = np.zeros((3,Ns[0],Ns[1]))\n",
    "    A_obs_outcome[0,:,:2] = 1\n",
    "    A_obs_outcome[:,:,2] = np.array([[0,0],   # If we choose left, what is the probability of achieving win / loss \n",
    "                             [pWin, 1-pWin],\n",
    "                             [1-pWin,pWin]]) # Choice gives an observable outcome\n",
    "                   # If true = left, right\n",
    "    A_obs_outcome[:,:,3] = np.array([[0,0],     # If we choose right, what is the probability of achieving win / loss \n",
    "                                     [1-pWin, pWin],\n",
    "                                     [pWin,1-pWin]]) # Choice gives an observable outcome\n",
    "                  # If true = left, right\n",
    "    \n",
    "    # 3. Mapping from mouse position states to observed mouse position\n",
    "    #\n",
    "    # [ .  .  .  .] start\n",
    "    # [ .  .  .  .] hint\n",
    "    # [ .  .  .  .] choose left         Row = Behaviour state\n",
    "    # [ .  .  .  .] choose right\n",
    "    #  s   h  l  r\n",
    "    #\n",
    "    # 3rd dimension = observed behaviour\n",
    "    # The 2nd dimension maps the dependance on cheese state (unvariant)\n",
    "    A_obs_behaviour = np.zeros((Ns[1],Ns[0],Ns[1]))\n",
    "    for i in range (Ns[1]) :\n",
    "        A_obs_behaviour[i,:,i] = np.array([1,1])\n",
    "    a = [A_obs_hints,A_obs_outcome,A_obs_behaviour]\n",
    "\n",
    "    No = [ai.shape[0] for ai in a] # Number of outcomes\n",
    "\n",
    "    # Finally, we set up the preferences of the environment (this is an environment, thus this is empty) ...\n",
    "    c = [np.zeros((No[0],T)),np.zeros((No[1],T)),np.zeros((No[2],T))]\n",
    "    # ... as well as the allowable transitions the mouse can choose :\n",
    "    u = np.array([[0,0],[0,1],[0,2],[0,3]]).astype(int)\n",
    "    \n",
    "    # Habits\n",
    "    e = np.ones((u.shape[0],))\n",
    "\n",
    "    # The environment has been well defined and we may now build a mdp_layer using the following constructor : \n",
    "    layer = mdp_layer(\"T-maze_environment\",\"process\",a,b,c,d,e,u,T)\n",
    "    #     mdp_layer(name of the layer,process or model, a,b,c,d,e,u,T)\n",
    "    print(\"Done.\")\n",
    "    return layer\n",
    "\n",
    "# We can test that the layer was well defined by instantiating and building it :\n",
    "tmaze_environment = build_tmaze_process(0.5,1.0,1.0)\n",
    "print(tmaze_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can see a general overview of the layer we just defined. One advantage of using the same object for processes and models is that we can easily use the same object for both purposes if needed. Let's now define the model our mouse is going to entertain (don't worry, it'll be much quicker :) ) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-maze gen. model set-up ...  Done.\n",
      "LAYER mouse_model : \n",
      " -------------------------------------\n",
      "LAYER DIMENSION REPORT (mouse_model): \n",
      "\n",
      "Observation modalities : 3\n",
      "    Modality 0 : 3 outcomes.\n",
      "    Modality 1 : 3 outcomes.\n",
      "    Modality 2 : 4 outcomes.\n",
      "Hidden states factors : 2\n",
      "    Model factor 0 : 2 possible states. \n",
      "    Model factor 1 : 4 possible states. \n",
      "Number of potential actions : 4\n",
      "    Factor 0 : 1 possible transitions. \n",
      "    Factor 1 : 4 possible transitions. \n",
      "-------------------------------------\n",
      "\n",
      "##################################################\n",
      "Layer weights :\n",
      "   Matrix a :\n",
      "     Modality 0 :\n",
      "[[[200.     0.   200.   200.  ]\n",
      "  [200.     0.   200.   200.  ]]\n",
      "\n",
      " [[  0.     0.25   0.     0.  ]\n",
      "  [  0.     0.25   0.     0.  ]]\n",
      "\n",
      " [[  0.     0.25   0.     0.  ]\n",
      "  [  0.     0.25   0.     0.  ]]]\n",
      "     Modality 1 :\n",
      "[[[200. 200.   0.   0.]\n",
      "  [200. 200.   0.   0.]]\n",
      "\n",
      " [[  0.   0. 200.   0.]\n",
      "  [  0.   0.   0. 200.]]\n",
      "\n",
      " [[  0.   0.   0. 200.]\n",
      "  [  0.   0. 200.   0.]]]\n",
      "     Modality 2 :\n",
      "[[[200.   0.   0.   0.]\n",
      "  [200.   0.   0.   0.]]\n",
      "\n",
      " [[  0. 200.   0.   0.]\n",
      "  [  0. 200.   0.   0.]]\n",
      "\n",
      " [[  0.   0. 200.   0.]\n",
      "  [  0.   0. 200.   0.]]\n",
      "\n",
      " [[  0.   0.   0. 200.]\n",
      "  [  0.   0.   0. 200.]]]\n",
      "   Matrix b :\n",
      "     Factor : 0  --- Transition 0 :\n",
      "[[200   0]\n",
      " [  0 200]]\n",
      "     Factor : 1  --- Transition 0 :\n",
      "[[200. 200. 200. 200.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]]\n",
      "     Factor : 1  --- Transition 1 :\n",
      "[[  0. 200. 200. 200.]\n",
      " [200.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]]\n",
      "     Factor : 1  --- Transition 2 :\n",
      "[[  0.   0. 200. 200.]\n",
      " [  0.   0.   0.   0.]\n",
      " [200. 200.   0.   0.]\n",
      " [  0.   0.   0.   0.]]\n",
      "     Factor : 1  --- Transition 3 :\n",
      "[[  0.   0. 200. 200.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [200. 200.   0.   0.]]\n",
      "   Matrix c :\n",
      "Modality 0 :\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Modality 1 :\n",
      "[[ 0.   0.   0. ]\n",
      " [ 0.   3.   1.5]\n",
      " [ 0.  -2.  -2. ]]\n",
      "Modality 2 :\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "   Matrix d :\n",
      "     Factor 0 :\n",
      "[0.25 0.25]\n",
      "     Factor 1 :\n",
      "[1 0 0 0]\n",
      "   Matrix e :\n",
      "[1. 1. 1. 1.]\n",
      "   Allowable actions u :\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 3]]\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_mouse_model(true_process_layer,la,rs,T_horizon,\n",
    "                      initial_clue_confidence = 0.1,action_selection_temperature = 32,mem_loss=0.0):\n",
    "    \"\"\"\n",
    "    true_process_layer : the mdp_layer object where the tmaze environment has been defined\n",
    "    la : how much the mouse is afraid of adverse outcomes (>0)\n",
    "    rs : how much the mouse wants to observe cheese (>0)\n",
    "    T_horizon : how much into the future the mouse will plan before picking its next action\n",
    "    initial_clue_confidence : how much the mouse knows about the clue reliability\n",
    "    \"\"\"\n",
    "    print(\"T-maze gen. model set-up ...  \",end='')\n",
    "    T = 3\n",
    "\n",
    "    #  The mouse knows where it stands in the maze initially, but it doesn't know where the cheese will spawn : this is something that\n",
    "    # it will need to learn !\n",
    "    d = [np.array([0.25,0.25]),np.array([1,0,0,0])]\n",
    "\n",
    "    \n",
    "    # Transition matrixes between hidden states ( = control states)\n",
    "    b=[]\n",
    "    for b_fac_proc in (true_process_layer.b):\n",
    "        b.append(np.copy(b_fac_proc)*200)\n",
    "    # The mouse knows how its action will affect the general situation. The mouse does not need\n",
    "    # to learn that element . Be aware that too much uncertainty in some situations may prove hard to resolve for our\n",
    "    # artifical subjects.\n",
    "\n",
    "\n",
    "    a = []\n",
    "    for a_mod_proc in (true_process_layer.a):\n",
    "        a.append(np.copy(a_mod_proc)*200)\n",
    "    a[0][:,:,1] = initial_clue_confidence*np.array([[0,0],\n",
    "                                                    [0.25,0.25],\n",
    "                                                    [0.25,0.25]])  \n",
    "    # The mouse already knows how the cheese position and its own position in the \n",
    "    # maze relates relates to its probability to observe cheese. It also knows where\n",
    "    # it is in the maze at all times. It knows this because it knows where it isn't ;)\n",
    "    # However, the mouse still has to learn the reliability of the clue.\n",
    "\n",
    "\n",
    "    # Finally, the preferences of the mouse are governed by the experimenter through the rs/la weights.\n",
    "    No = [ai.shape[0] for ai in a]\n",
    "\n",
    "    C_hints = np.zeros((No[0],T))\n",
    "    C_win_loss = np.zeros((No[1],T))\n",
    "    C_win_loss = np.array([[0,0,0],     #null\n",
    "                           [0,rs,rs/2.0],  #win : as you can see, the mouse would much rather find the cheese at timestep 2 rather than 3. Feel free to play with this factor.\n",
    "                           [0,-la,-la]]) #loss\n",
    "    C_observed_behaviour = np.zeros((No[2],T))\n",
    "    c = [C_hints,C_win_loss,C_observed_behaviour]\n",
    "    # The mouse has no preference towards seeing a clue or being in a given position. However, it does have a preference regarding\n",
    "    # the outcome of the trial (i.e. seeing the cheese or the mousetrap)\n",
    "    \n",
    "    # The allowable actions have been defined earlier\n",
    "    u = true_process_layer.U\n",
    "    # u = np.array([[0,0],[0,1],[0,2],[0,3]]).astype(int)\n",
    "    \n",
    "    # Habits\n",
    "    e = np.ones((u.shape[0],))\n",
    "\n",
    "    layer = mdp_layer(\"mouse_model\",\"model\",a,b,c,d,e,u,T,T_horiz=T_horizon)\n",
    "    # This time, we define our layer as a \"model\" \n",
    "\n",
    "    # Here, we give a few hyperparameters guiding the beahviour of our agent :\n",
    "    layer.hyperparams.alpha = action_selection_temperature # action precision : \n",
    "        # for high values the mouse will always perform the action it perceives as optimal, with very little exploration \n",
    "        # towards actions with similar but slightly lower interest\n",
    "\n",
    "    layer.learn_options.eta = 1 # learning rate (shared by all channels : a,b,c,d,e)\n",
    "    layer.learn_options.learn_a = True  # The agent learns the reliability of the clue\n",
    "    layer.learn_options.learn_b = False # The agent does not learn transitions\n",
    "    layer.learn_options.learn_d = True  # The agent has to learn the initial position of the cheese\n",
    "    layer.learn_options.backwards_pass = True  # When learning, the agent will perform a backward pass, using its perception of \n",
    "                                               # states in later trials (e.g. I saw that the cheese was on the right at t=3)\n",
    "                                               # as well as what actions it performed (e.g. and I know that the cheese position has\n",
    "                                               # not changed between timesteps) to learn more reliable weights (therefore if my clue was\n",
    "                                               # a right arrow at time = 2, I should memorize that cheese on the right may correlate with\n",
    "                                               # right arrow in general)\n",
    "    layer.learn_options.memory_loss = mem_loss\n",
    "                                            # How many trials will be needed to \"erase\" 50% of the information gathered during one trial\n",
    "                                            # Used during the learning phase\n",
    "    print(\"Done.\")\n",
    "    return layer\n",
    "\n",
    "mouse_model = build_mouse_model(tmaze_environment,2,3,3,1.0)\n",
    "print(mouse_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our environment (generative process) and the model our mouse will entertain (generative model), we need to describe how the two will interact to form a system. \n",
    "\n",
    "To create interactions between layers, we need to establish *links* between some of their inputs and outputs :\n",
    "- The environment outputs (outcomes) are forwarded to the mouse sensory states (observations)\n",
    "- The mouse actions (active states) lead to a changes in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actynf import link # Let's import the \"link\" component to connect layers between one another !\n",
    "\n",
    "#1. Create a link from observations generated by the environment to the mouse sensory states :\n",
    "mouse_model.inputs.o = link(tmaze_environment, lambda x : x.o)\n",
    "  #     the layer from which we get the data | the function extracting the data\n",
    "\n",
    "#2. Create a link from the actions selected by the mouse to the t-maze environment :\n",
    "tmaze_environment.inputs.u = link(mouse_model,lambda x : x.u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step, we have designed the following construction :\n",
    "\n",
    "![tmaze_network](local_resources/tmaze/tmaze_network.png)\n",
    "\n",
    "<sup><sub>Note that in this situation, the mouse model of the maze is very close to the true environment. However, as long as the sizes of the *linked* dimensions (here \"mouse_model.u-tmaze_environment.u\" and \"mouse_model.o-tmaze_environment.o\") match, one could link very different layers.</sub></sup>\n",
    "\n",
    "To use the resulting interconnected system, we will need a dedicated *actynf* object called a *network*.\n",
    "\n",
    "*actynf* uses the *network* class to encapsulate *mdp_layers* and their *links*. When the network has been defined, we wil be able to run the whole thing !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actynf.architecture.network import network\n",
    "\n",
    "# tmaze_net = network([tmaze_environment,mouse_model],\"t-maze_network\")\n",
    "\n",
    "# For future use, let's also create a function that builds the whole network from scratch using the previously explicited set of parameters :\n",
    "def build_tmaze_network(pinit,pHA,pWin,la,rs,T_horizon,initial_clue_confidence,action_selection_temp=32,memory_loss=0.0,\n",
    "                            name=\"t-maze_network\"):\n",
    "    \n",
    "    _tmaze_environment = build_tmaze_process(pinit,pHA,pWin)\n",
    "    _mouse_model = build_mouse_model(_tmaze_environment,la,rs,T_horizon,\n",
    "                                    initial_clue_confidence,action_selection_temp,memory_loss)\n",
    "\n",
    "    _mouse_model.inputs.o = link(_tmaze_environment, lambda x : x.o)\n",
    "    _tmaze_environment.inputs.u = link(_mouse_model,lambda x : x.u)\n",
    "\n",
    "    return network([_tmaze_environment,_mouse_model],name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Running simulations with the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One trial\n",
      "T-maze gen. process set-up ...  Done.\n",
      "T-maze gen. model set-up ...  Done.\n",
      " Network [t-maze_network] : Timestep 3 / 3\n",
      " Done !   -------- (seeds : [7829-0;2548-0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<actynf.layer.model_layer.layer_STM at 0x1e67fc7b850>,\n",
       "  <actynf.layer.model_layer.layer_STM at 0x1e67fc7ae30>],\n",
       " [{'seed': [array(7829), array(1)],\n",
       "   'a': [array([[[1. , 0. , 1. , 1. ],\n",
       "            [1. , 0. , 1. , 1. ]],\n",
       "    \n",
       "           [[0. , 0.9, 0. , 0. ],\n",
       "            [0. , 0.1, 0. , 0. ]],\n",
       "    \n",
       "           [[0. , 0.1, 0. , 0. ],\n",
       "            [0. , 0.9, 0. , 0. ]]]),\n",
       "    array([[[1.  , 1.  , 0.  , 0.  ],\n",
       "            [1.  , 1.  , 0.  , 0.  ]],\n",
       "    \n",
       "           [[0.  , 0.  , 0.99, 0.01],\n",
       "            [0.  , 0.  , 0.01, 0.99]],\n",
       "    \n",
       "           [[0.  , 0.  , 0.01, 0.99],\n",
       "            [0.  , 0.  , 0.99, 0.01]]]),\n",
       "    array([[[1., 0., 0., 0.],\n",
       "            [1., 0., 0., 0.]],\n",
       "    \n",
       "           [[0., 1., 0., 0.],\n",
       "            [0., 1., 0., 0.]],\n",
       "    \n",
       "           [[0., 0., 1., 0.],\n",
       "            [0., 0., 1., 0.]],\n",
       "    \n",
       "           [[0., 0., 0., 1.],\n",
       "            [0., 0., 0., 1.]]])],\n",
       "   'b': [array([[[1],\n",
       "            [0]],\n",
       "    \n",
       "           [[0],\n",
       "            [1]]]),\n",
       "    array([[[1., 0., 0., 0.],\n",
       "            [1., 1., 0., 0.],\n",
       "            [1., 1., 1., 1.],\n",
       "            [1., 1., 1., 1.]],\n",
       "    \n",
       "           [[0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0.],\n",
       "            [0., 0., 0., 0.],\n",
       "            [0., 0., 0., 0.]],\n",
       "    \n",
       "           [[0., 0., 1., 0.],\n",
       "            [0., 0., 1., 0.],\n",
       "            [0., 0., 0., 0.],\n",
       "            [0., 0., 0., 0.]],\n",
       "    \n",
       "           [[0., 0., 0., 1.],\n",
       "            [0., 0., 0., 1.],\n",
       "            [0., 0., 0., 0.],\n",
       "            [0., 0., 0., 0.]]])],\n",
       "   'c': [array([[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]),\n",
       "    array([[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]),\n",
       "    array([[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]])],\n",
       "   'd': [array([0.8, 0.2]), array([1, 0, 0, 0])],\n",
       "   'e': array([1., 1., 1., 1.]),\n",
       "   'u': array([[0, 0],\n",
       "          [0, 1],\n",
       "          [0, 2],\n",
       "          [0, 3]]),\n",
       "   'params': <actynf.layer.parameters.hyperparameters.hyperparameters at 0x1e67fc7bb20>,\n",
       "   'learn_params': <actynf.layer.parameters.learning_parameters.learning_parameters at 0x1e67fc7bfd0>},\n",
       "  {'seed': [array(2548), array(1)],\n",
       "   'a': [array([[[201.49,   0.  , 200.99, 200.  ],\n",
       "            [200.51,   0.  , 200.01, 200.  ]],\n",
       "    \n",
       "           [[  0.  ,   0.25,   0.  ,   0.  ],\n",
       "            [  0.  ,   0.25,   0.  ,   0.  ]],\n",
       "    \n",
       "           [[  0.  ,   0.25,   0.  ,   0.  ],\n",
       "            [  0.  ,   0.25,   0.  ,   0.  ]]]),\n",
       "    array([[[201.49, 200.  ,   0.  ,   0.  ],\n",
       "            [200.51, 200.  ,   0.  ,   0.  ]],\n",
       "    \n",
       "           [[  0.  ,   0.  , 198.99,   2.  ],\n",
       "            [  0.  ,   0.  ,   2.01, 198.  ]],\n",
       "    \n",
       "           [[  0.  ,   0.  ,   2.  , 198.  ],\n",
       "            [  0.  ,   0.  , 198.  ,   2.  ]]]),\n",
       "    array([[[201.49,   0.  ,   0.  ,   0.  ],\n",
       "            [200.51,   0.  ,   0.  ,   0.  ]],\n",
       "    \n",
       "           [[  0.  , 200.  ,   0.  ,   0.  ],\n",
       "            [  0.  , 200.  ,   0.  ,   0.  ]],\n",
       "    \n",
       "           [[  0.  ,   0.  , 200.99,   0.  ],\n",
       "            [  0.  ,   0.  , 200.01,   0.  ]],\n",
       "    \n",
       "           [[  0.  ,   0.  ,   0.  , 200.  ],\n",
       "            [  0.  ,   0.  ,   0.  , 200.  ]]])],\n",
       "   'b': [array([[[200],\n",
       "            [  0]],\n",
       "    \n",
       "           [[  0],\n",
       "            [200]]]),\n",
       "    array([[[200.,   0.,   0.,   0.],\n",
       "            [200., 200.,   0.,   0.],\n",
       "            [200., 200., 200., 200.],\n",
       "            [200., 200., 200., 200.]],\n",
       "    \n",
       "           [[  0., 200.,   0.,   0.],\n",
       "            [  0.,   0.,   0.,   0.],\n",
       "            [  0.,   0.,   0.,   0.],\n",
       "            [  0.,   0.,   0.,   0.]],\n",
       "    \n",
       "           [[  0.,   0., 200.,   0.],\n",
       "            [  0.,   0., 200.,   0.],\n",
       "            [  0.,   0.,   0.,   0.],\n",
       "            [  0.,   0.,   0.,   0.]],\n",
       "    \n",
       "           [[  0.,   0.,   0., 200.],\n",
       "            [  0.,   0.,   0., 200.],\n",
       "            [  0.,   0.,   0.,   0.],\n",
       "            [  0.,   0.,   0.,   0.]]])],\n",
       "   'c': [array([[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]),\n",
       "    array([[ 0. ,  0. ,  0. ],\n",
       "           [ 0. ,  3. ,  1.5],\n",
       "           [ 0. , -2. , -2. ]]),\n",
       "    array([[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]])],\n",
       "   'd': [array([0.75, 0.75]), array([2, 0, 0, 0])],\n",
       "   'e': array([1., 1., 1., 1.]),\n",
       "   'u': array([[0, 0],\n",
       "          [0, 1],\n",
       "          [0, 2],\n",
       "          [0, 3]]),\n",
       "   'params': <actynf.layer.parameters.hyperparameters.hyperparameters at 0x1e67fc7a380>,\n",
       "   'learn_params': <actynf.layer.parameters.learning_parameters.learning_parameters at 0x1e67fc797b0>}])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run our network for a single trial : \n",
    "print(\"One trial\")\n",
    "tmaze_net = build_tmaze_network(0.8,0.9,0.99,2,3,2,1,action_selection_temp=32,memory_loss=0.0,\n",
    "                            name=\"t-maze_network\")\n",
    "tmaze_net.run()\n",
    "\n",
    "# # Or for multiple trials : (careful, this is the same object, meaning that the mouse model at the start of the 10 trials will be based on its previous experience during the one trial !)\n",
    "# N = 10\n",
    "# print(str(N) + \" trials\")\n",
    "# tmaze_net.run_N_trials(10)\n",
    "# N = 10\n",
    "# for trial_idx in range(N):\n",
    "#     tmaze_net.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : seeding\n",
    "*actynf* is based on Markov Decision Processes which rely heavily on random samplings :\n",
    "-  for generative processes, during states transition and observations generation\n",
    "-  for generative models, during the action selection (and possibly during the planning phase on the modified Sophisticated Inference algorithm)\n",
    "To ensure our ability to reproduce the results of each trial, we use pseudo-random number generators. \n",
    "\n",
    "Using seeds, we can ensure that samplings will always lead to the same results. Here, the seed of the last generation as well as the number of successive trials done with this seed are shown :\n",
    "\n",
    "``` >> (seeds : [layer_0_seed-trials_since_seeding;layer_1_seed-trials_since_seeding])```\n",
    "\n",
    "You may (re)seed a layer or a network using the reseed function : \n",
    "``` \n",
    "> my_layer.reseed(new_seed[optional])\n",
    "> my_network.reseed(new_seed[optional])\n",
    "```\n",
    "<sup><sub>If you dont provide a new seed, the layer will just reinitialize the random number generator.</sub></sup>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : accessing the results of simulations\n",
    "\n",
    "*actynf* *layers* save the results of the last trial's computations in their **Short Term Memory (STM)**. This is useful during inference, learning and inspection.\n",
    "\n",
    "STM stores two types of values:\n",
    "- **Definite variables :** fixed values that are perceived as having definetely happened ; *observations (o) , states(x) and actions(u).*\n",
    "- **Distributions :** probability distributions, noted computationally using the suffix \"_d\". Those are multidimensional matrices that represent joint probability distributions over a set of states/observations/action selection (usually posterior estimates, but may also be generative ditributions) ; *observations (o_d) , states(x_d) and actions (u_d).*\n",
    "\n",
    "<sup><sub>*actynf* uses distributions rather than definite outcomes for all computations (generate observations/state transitions/perception). As an example, if the generative process layer outputs a specific set of observations for a given timestep ```o``` (say, $o_{t=1} = [2,0,1]$), it will automatically be converted to a multidimensionnal joint probability distribution ```o_d``` (in this case, a 3x3x4 matrix where all weights are 0.0 except for the one with (Python) coordinates [2,0,1] with weight 1.0). </sub></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can manually check the content of a layer's short term memory :\n",
    "- Through  the layer object directly : ```> result_stm = tmaze_environment.STM ```\n",
    "- Through the *network* layer components : ```> result_stm = tmaze_net.layers[0].STM ```\n",
    "- Through the run method(s) of the *network* : ```> result_stm = tmaze_net.run(return_STMs=True) ```\n",
    "\n",
    "**Warning /!\\ :** Each time time a layer starts a new trial, a new STM object is generated. Therefore, you can't just use a STM \"pointer\" that you keep across several trials. Instead, you should collect STMs after each trial iteration (or use the \"return_STMs\" option in the *network.run_N_trials* method to gather all the generated STMs).\n",
    "\n",
    "You can then extract relevant variables from this STM object : ```o = result_stm.o ; o_d = result_stm.o_d #etc...```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results of the previous run for the mouse perception of the cheese position ! \n",
      "    Perception of cheese on the left across time : [0.5  0.99 0.99] ( True values : [1. 1. 1.] ).\n",
      "    Perception of cheese on the right across time : [0.5  0.01 0.01] ( True values : [0. 0. 0.] ).\n"
     ]
    }
   ],
   "source": [
    "# Let's check the results of the previous run :\n",
    "tmaze_environment_STM = tmaze_net.layers[0].STM\n",
    "mouse_model_STM = tmaze_net.layers[1].STM\n",
    "\n",
    "# Example : let's see how the mouse perception of the cheese position changed during the last trial !\n",
    "print(\"Here are the results of the previous run for the mouse perception of the cheese position ! \")\n",
    "# This is what the mouse infered as hidden states :\n",
    "joint_state_perception = (mouse_model_STM.x_d) # x_d has dimensions Nstates_factor_1 x Nstates_factor_2 x ... x Nstates_factor_n x T\n",
    "# And this is the true state distribution :\n",
    "true_state_distribution = tmaze_environment_STM.x_d\n",
    "\n",
    "# To marginalize this joint distribution over the axis of the cheese position (0), we just need to sum over the other state dimensions !\n",
    "marginalized_mouse_model_of_cheese = np.sum(joint_state_perception, (1,))\n",
    "marginalized_true_side_of_cheese = np.sum(true_state_distribution, (1,))\n",
    "possible_cheese_position = [\"left\",\"right\"]  # Not yet any implemented support for labels :/\n",
    "for cheese_pos_idx in range(len(possible_cheese_position)):\n",
    "    print(\"    Perception of cheese on the \" + possible_cheese_position[cheese_pos_idx] + \" across time : \" + str(marginalized_mouse_model_of_cheese[cheese_pos_idx,:]) + \" ( True values : \" + str(marginalized_true_side_of_cheese[cheese_pos_idx,:]) + \" ).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(You may notice that the mouse is very naive about the cheese position, even after seing it at the last timestep : it does not reminisce about the cheese position in timesteps 0 & 1. This can be changed by using some form of \"backward pass\" to update those posteriors given the new information obtained and the knowledge about the actions performed. We implemented this sort of algorithm during subject learning. The result of this backward pass is available in the STM's \"x_smoothed_d\" variable)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Plotting nice figures :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a bunch of data and make nice plots !\n",
    "To generate data for each set of parameters, let's use the functions we defined above and wrap them in a single function that generates numpy arrays of simulated data for several trials depending on a large number of parameters !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-maze gen. process set-up ...  Done.\n",
      "T-maze gen. model set-up ...  Done.\n",
      "(array([[1.],\n",
      "       [1.],\n",
      "       [1.]]), array([[0.],\n",
      "       [1.],\n",
      "       [2.]]), array([[0.],\n",
      "       [2.],\n",
      "       [0.]]), array([[[0.5],\n",
      "        [0.5],\n",
      "        [0. ]],\n",
      "\n",
      "       [[0.5],\n",
      "        [0.5],\n",
      "        [1. ]]]), array([[1.],\n",
      "       [2.]]), array([[[0.1573262 ],\n",
      "        [0.16666667]],\n",
      "\n",
      "       [[0.3332885 ],\n",
      "        [0.16666667]],\n",
      "\n",
      "       [[0.25469265],\n",
      "        [0.33333333]],\n",
      "\n",
      "       [[0.25469265],\n",
      "        [0.33333333]]]))\n"
     ]
    }
   ],
   "source": [
    "from actynf.base.function_toolbox import normalize\n",
    "\n",
    "def generate_data(N,pinit,pHA,pWin,la,rs,T_horizon,initial_clue_confidence,\n",
    "                  Nswitch=100,pinit2=1.0,\n",
    "                  action_selection_temp=32,memory_loss=0.0,\n",
    "                  return_ds = False,verbose=False):\n",
    "    T = 3\n",
    "    # Build our network\n",
    "    network_tmaze = build_tmaze_network(pinit,pHA,pWin,la,rs,T_horizon,initial_clue_confidence,action_selection_temp,memory_loss,name=\"t-maze_network\")\n",
    "    \n",
    "    # Initialize a few arrays to store the results of our simulations\n",
    "    true_cheese_state = np.zeros((T,N))\n",
    "    true_mouse_state = np.zeros((T,N))\n",
    "    clue_observations = np.zeros((T,N))\n",
    "\n",
    "    mouse_cheese_perception = np.zeros((2,T,N))\n",
    "    mouse_actions = np.zeros((T-1,N))\n",
    "    mouse_action_posterior = np.zeros((4,T-1,N))\n",
    "\n",
    "    if return_ds :\n",
    "        cheese_starting_pos = np.zeros((2,N+1))\n",
    "        cheese_starting_pos_perception = np.zeros((2,N+1))\n",
    "        cheese_starting_pos[:,0] = network_tmaze.layers[0].d[0]\n",
    "        cheese_starting_pos_perception[:,0] = normalize(network_tmaze.layers[1].d[0])\n",
    "    \n",
    "    for trial_idx in range(N):\n",
    "        # We can change the paradigm mid-simulation to explore how the agent would react !\n",
    "        # If Nswitch > N, the switch never happens !\n",
    "        if trial_idx == Nswitch:\n",
    "            network_tmaze.layers[0].d[0] = np.array([pinit2,1-pinit2])\n",
    "        \n",
    "\n",
    "        # Run the simulation for trial_idx\n",
    "        network_tmaze.run(verbose=verbose)\n",
    "        \n",
    "        # 1.Fetch the layers memories for this idx : \n",
    "        tmaze_environment_stm = network_tmaze.layers[0].STM\n",
    "        mouse_model_stm =  network_tmaze.layers[1].STM\n",
    "\n",
    "        # 2.a. Save some environment variables\n",
    "        clue_observations[:,trial_idx] = tmaze_environment_stm.o[0,:]\n",
    "        true_cheese_state[:,trial_idx] = tmaze_environment_stm.x[0,:]\n",
    "        true_mouse_state[:,trial_idx] = tmaze_environment_stm.x[1,:]        \n",
    "\n",
    "        # 2.b. Save some mouse model variables\n",
    "        mouse_cheese_perception_for_this_trial = np.sum(mouse_model_stm.x_d,axis = 1)\n",
    "        mouse_cheese_perception[:,:,trial_idx] = mouse_cheese_perception_for_this_trial\n",
    "        mouse_actions[:,trial_idx] = mouse_model_stm.u\n",
    "        mouse_action_posterior[:,:,trial_idx] = mouse_model_stm.u_d\n",
    "\n",
    "        if return_ds :\n",
    "            cheese_starting_pos[:,trial_idx+1] = network_tmaze.layers[0].d[0]\n",
    "            cheese_starting_pos_perception[:,trial_idx+1] = normalize(network_tmaze.layers[1].d[0])\n",
    "\n",
    "    if return_ds:\n",
    "        return true_cheese_state,true_mouse_state,clue_observations,mouse_cheese_perception,mouse_actions,mouse_action_posterior,cheese_starting_pos,cheese_starting_pos_perception\n",
    "    return true_cheese_state,true_mouse_state,clue_observations,mouse_cheese_perception,mouse_actions,mouse_action_posterior\n",
    "        \n",
    "\n",
    "# We can see what it outputs : \n",
    "print(generate_data(1,0.5,0.8,1,1,2,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import a few functions that help us plot animations / graphs of the various results !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.31\n"
     ]
    }
   ],
   "source": [
    "# To generate animation gifs : \n",
    "print(actynf.__version__)\n",
    "from actynf.demo_tools.tmaze.plot import tmaze_drawer\n",
    "\n",
    "def draw_and_save_tmaze_gif(N = 30,pinit = 0.85,pHA = 1.0,\n",
    "        pWin = 0.98,la = 2,rs = 3,T_horizon = 2,initial_clue_confidence = 1.0):\n",
    "    ressources_path = os.path.join(\"ressources/tmaze\")\n",
    "    drawer = tmaze_drawer(ressources_path)\n",
    "        # The drawer is a class that allows us to draw a set of\n",
    "        # pictures of the maze ! All we need are the indicators \n",
    "        # generated by the \"generate_data\" function defined before\n",
    "\n",
    "    true_cheese_state,true_mouse_state,clue_observations,mouse_cheese_perception,mouse_actions,mouse_action_posterior = generate_data(N,pinit,pHA,pWin,la,rs,T_horizon,initial_clue_confidence)\n",
    "    my_gif = []\n",
    "    for trial in range(N):\n",
    "        print(\"Trial \" + str(trial+1) + \" / \" + str(N))\n",
    "        true_reward_state = true_cheese_state[:,trial]\n",
    "        true_agent_state = true_mouse_state[:,trial]\n",
    "        cheese_perception = mouse_cheese_perception[:,:,trial]\n",
    "        clue_obs = clue_observations[:,trial]\n",
    "        imglist = drawer.get_trial_mazeplot(trial+1,N,\n",
    "                              15,true_reward_state,true_agent_state,\n",
    "                              cheese_perception,clue_obs,\n",
    "                              None,None)\n",
    "        my_gif += imglist\n",
    "    my_gif[0].save(\"ressources/tmaze/renders/render.gif\", save_all=True, append_images=my_gif[1:],duration=30,loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reliable clue, random environment       | Reliable clue , environment stabilizes after trial 10 | Unreliable clue & random environment\n",
    ":--------------------------------------:|:------------------------------------:|:------------------------------------:\n",
    "![Image1](local_resources/tmaze/renders/render_good_clue_2.gif) |![Image2](local_resources/tmaze/renders/render_good_clue_cheese_stabilizes_at_10.gif)|![Image3](local_resources/tmaze/renders/render_bad_clue_random_env.gif)\n",
    "$p_{init} = 0.5$ , $pHA = 1.0$|$p_{init} = 0.5$ for the first 10 trials, then $p_{init} = 0.05$ , $pHA = 1.0$|$p_{init} = 0.5$ , $pHA = 0.5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the behaviour of our mouze on a \"classic\" matplotlib figure !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-maze gen. process set-up ...  Done.\n",
      "T-maze gen. model set-up ...  Done.\n",
      "Simulations completed : drawing the figure !\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annic\\AppData\\Local\\Temp\\ipykernel_16436\\4122802752.py:81: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  myfig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABRlklEQVR4nO3de1xUdfoH8M/hDnITDZBEQDRD80qK2MVUCtNK23bVtkJN3c3NTNlq9bcpGRWtlbeiKAvpuqKb2VaupSSailoi5ZW8oJAKXggGUEBmvr8/iMmRAeYMc5g5h8/79ZoXcM73PN/n3GYezpyLJIQQICIiItIIJ3snQERERGRLLG6IiIhIU1jcEBERkaawuCEiIiJNYXFDREREmsLihoiIiDSFxQ0RERFpiou9E2hrBoMBZ86cgY+PDyRJsnc6REREZAEhBCoqKhASEgInp+aPzbS74ubMmTMIDQ21dxpERERkhaKiInTt2rXZNu2uuPHx8QFQv3B8fX3tnA0RERFZQqfTITQ01Pg53px2V9w0fBXl6+tr++JGGIDKE8AVHeDqC3h3ByQbn9akhT60MA/sg30o1Qc5Dq1sU1p4T7+KJaeUtLviRhFXKoDj7wL5rwNVBb8P9+4O3PAEEDkNcG250tR8H1qYB/bBPpTqgxyHVrYpLbynW0lqbw/O1Ol08PPzQ3l5uW2O3FQVAd+OAiqO/Tbg6sX5W3Xp0wMYmQV0sPJcHy30oYV5YB/sQ6k+yHFoZZvSwnv6NeR8fvN4amtcqahfuZUFqF+x19aJvw2rLKhvd6WiffahhXlgH+xDqT7IcWhlm9LCe3or2bW42bZtG+69916EhIRAkiSsX7++xWmys7MxaNAguLu7o0ePHsjIyFA8zyYdf6++ahV1zbcTdfXtjqe3zz60MA/sg30o1Qc5Dq1sU1p4T28luxY3VVVV6N+/P1JTUy1qX1BQgLFjx2LEiBHIy8vDnDlzMH36dHz99dcKZ2qGMAD5K+RN8/OK+unaUx9amAf2wT6U6oMch1a2KS28p9uAXYubu+++Gy+88ALuv/9+i9qnpaUhIiICr732GqKiojBr1iz88Y9/xNKlS5ucpqamBjqdzuRlE5UnfjuBytJTlkT9NJUn2lcfWpgH9sE+lOqDHIdWtiktvKfbgKqulsrJyUFcXJzJsPj4eMyZM6fJaVJSUrBo0aJGw0+d7gkfnfW1nVvFFYRYMd2ZX4ai1se13fShhXlgH9ruY8SnT5lt28flF3zZUX4f93yQioN1zd9gjBxPW6xvLfRhz/3CUF1tcVtVnVBcXFyMoKAgk2FBQUHQ6XS4fPmy2Wnmz5+P8vJy46uoqMgmuRhcrHt0g5zptNCHFuaBfbTPPioNHlb1Ye10ZF9tsb610Ida9gtVFTfWcHd3N96wz5Y37qvzcMYVD2c5B+ZwxcMZdR7O7aoPLcwD+2iffRQaAlCoD4DBwk4MAjilD0ChIcDiPshxtMX61kIfatkvVFXcBAcHo6SkxGRYSUkJfH194enp2bbJSBIqQrxkTaK73guQ87BOLfShhXlgH+2yDwEnZFy+VVYfGZdvg1DX2yr9pi3Wtxb6UMt+oaq9MDY2FllZWSbDNm3ahNjYWLvkUxHsiTrPlv97FADqPJ1RGSS/ANNCH1qYB/bRPvtYUz0EJ/WdUSeaf6usE044qb8Oa6sHy+6DHEdbrG8t9KGG/cKuxU1lZSXy8vKQl5cHoP5S77y8PBQWFgKoP18mISHB2P6xxx7DiRMn8Mwzz+DIkSN48803sWbNGsydO9ce6UO4OKG4b4DxDbaJ2xihztMZxX0DIFzkL24t9KGFeWAf7bOPKuGBh8sfMx6Gv/ZQfMOwQn0nPFz+V1QJnm+jZm2xvrXQhxr2C7s+fiE7OxsjRoxoNHzy5MnIyMjAlClTcPLkSWRnZ5tMM3fuXBw6dAhdu3bFggULMGXKFIv7bLh980+HAuHjY5vaTqozwKf4MnzOXIJrtd44/IqHM3TXe6EyyNOqN1at9aGFeWAf2uujqaulrtZBqsYEjz2Y4rkd3ZxLjcNP6QOQcfk2rK0ezMJGQ9pifWuhj7beLwzV1Sic96xFj19ot8+WsmVxYyQEXKr1cKoTMLhI9ScvyjmPoL30oYV5YB+a6cOS4qaBBAO6OZXC26kalQYPFBoCeI6NhrXF+tZCH221X8gpblR1nxuHJ0mo81R4kWqhDy3MA/tol30IOOGUoTPAmxC3C22xvrXQhyPuF/yXg4iIiDSFxQ0RERFpCosbIiIi0hQWN0RERKQpLG6IiIhIU1jcEBERkaawuCEiIiJNYXFDREREmsLihoiIiDSFxQ0RERFpCosbIiIi0hQWN0RERKQpVhU3hYWFMPcwcSEECgsLW50UERERkbWsKm4iIiJw/vz5RsNLS0sRERHR6qSIiIiIrGVVcSOEgCRJjYZXVlbCw8Oj1UkRERERWctFTuPExEQAgCRJWLBgAby8vIzj9Ho9du/ejQEDBtg0QSIiIiI5ZBU3+/btA1B/5Gb//v1wc3MzjnNzc0P//v3x1FNP2TZDIiIiIhlkFTdbtmwBAEydOhXLly+Hr6+vIkkRERERWcuqc24WL17cZGGzf//+ViVERERE1BpWFTd9+/bFV1991Wj4q6++iiFDhrQ6KSIiIiJrWVXcJCYm4oEHHsDMmTNx+fJlnD59GqNGjcLixYvxySef2DpHIiIiIotZVdw888wzyMnJwXfffYd+/fqhX79+cHd3x08//YT777/f1jkSERERWczqxy/06NEDN910E06ePAmdToeJEyciODjYlrkRERERyWZVcbNjxw7069cPR48exU8//YS33noLTzzxBCZOnIhff/3V1jkSERERWcyq4mbkyJGYOHEidu3ahaioKEyfPh379u1DYWEh+vbta+sciYiIiCwm6z43Db755hsMHz7cZFhkZCR27NiBF1980SaJEREREVnDqiM3DYXNsWPH8PXXX+Py5csAfn8sAxEREZG9WFXcXLx4EaNGjcINN9yAMWPG4OzZswCAadOm8fELREREZFdWFTdz586Fq6srCgsLTR6eOXHiRPzvf/+zWXJEREREcll9zs3XX3+Nrl27mgzv2bMnTp06ZZPEiIiIiKxh1ZGbqqoqkyM2DUpLS+Hu7t7qpIiIiIisZVVxc9ttt+GDDz4w/i1JEgwGAxYvXowRI0bYLDkiIiIiuaz6Wmrx4sUYNWoUfvjhB9TW1uKZZ57BwYMHUVpaih07dtg6RyIiIiKLWXXk5qabbsLPP/+MW2+9FePGjUNVVRX+8Ic/YN++fYiMjJQdLzU1FeHh4fDw8EBMTAz27NnTZNuMjAxIkmTy8vDwsGY2iIiISIOsOnJTWFiI0NBQ/POf/zQ7rlu3bhbHyszMRGJiItLS0hATE4Nly5YhPj4e+fn5CAwMNDuNr68v8vPzjX9LkiR/JoiIiEiTrDpyExERgfPnzzcafvHiRURERMiKtWTJEsyYMQNTp05F7969kZaWBi8vL6Snpzc5jSRJCA4ONr6CgoKabFtTUwOdTmfyIiIiIu2y6siNEMLs0ZLKykpZXxHV1tZi7969mD9/vnGYk5MT4uLikJOT0+R0lZWVCAsLg8FgwKBBg/DSSy+hT58+ZtumpKRg0aJFjYbPveVuuEhuFudKRBq10N4JEJGtySpuEhMTAfz+mIWrLwfX6/XYvXs3BgwYYHG8CxcuQK/XNzryEhQUhCNHjpidplevXkhPT0e/fv1QXl6OV199FcOGDcPBgwcb3XcHAObPn2/MGwB0Oh1CQ0MtzpGIiIjURVZxs2/fPgD1R272798PN7ffj3y4ubmhf//+ij9+ITY2FrGxsca/hw0bhqioKLz99ttITk5u1N7d3Z333iEiImpHZBU3W7ZsAQBMnToVy5cvh6+vb6s679y5M5ydnVFSUmIyvKSkBMHBwRbFcHV1xcCBA3Hs2LFW5UJERETaYNUJxatWrWp1YQPUH+2Jjo5GVlaWcZjBYEBWVpbJ0Znm6PV67N+/H126dGl1PkRERKR+Vp1QXFVVhZdffhlZWVk4d+4cDAaDyfgTJ05YHCsxMRGTJ0/GzTffjCFDhmDZsmWoqqrC1KlTAQAJCQm4/vrrkZKSAgB4/vnnMXToUPTo0QNlZWV45ZVXcOrUKUyfPt2aWSEiIiKNsaq4mT59OrZu3YpHHnkEXbp0adV9ZiZOnIjz589j4cKFKC4uxoABA7Bx40bjScaFhYVwcvr9ANOvv/6KGTNmoLi4GB07dkR0dDR27tyJ3r17W50DERERaYckhBByJ/L398dXX32FW265RYmcFKXT6eDn54dRHSfzUnAiQv7CXvZOgYgsYKiuRuG8Z1FeXt7iqTFWnXPTsWNHBAQEWJUcERERkZKsKm6Sk5OxcOFCXLp0ydb5EBEREbWKVefcvPbaazh+/DiCgoIQHh4OV1dXk/G5ubk2SY6IiIhILquKm/Hjx9s4DSIiIiLbsKq4SUpKsnUeRERERDZhVXHTYO/evTh8+DAAoE+fPhg4cKBNkiIiIiKyllXFzblz5zBp0iRkZ2fD398fAFBWVoYRI0Zg9erVuO6662yZIxEREZHFrLpa6oknnkBFRQUOHjyI0tJSlJaW4sCBA9DpdJg9e7atcyQiIiKymFVHbjZu3IjNmzcjKirKOKx3795ITU3FXXfdZbPkiIiIiOSy6siNwWBodPk3UP+E7mufM0VERETUlqwqbkaOHIknn3wSZ86cMQ47ffo05s6di1GjRtksOSIiIiK5rCpu3njjDeh0OoSHhyMyMhKRkZGIiIiATqfD66+/busciYiIiCxm1Tk3oaGhyM3NxebNm3HkyBEAQFRUFOLi4myaHBEREZFcso7cfPvtt+jduzd0Oh0kScKdd96JJ554Ak888QQGDx6MPn364LvvvlMqVyIiIqIWySpuli1bhhkzZph91Lifnx/++te/YsmSJTZLjoiIiEguWcXNjz/+iNGjRzc5/q677sLevXtbnRQRERGRtWQVNyUlJWYvAW/g4uKC8+fPtzopIiIiImvJKm6uv/56HDhwoMnxP/30E7p06dLqpIiIiIisJau4GTNmDBYsWIDq6upG4y5fvoykpCTcc889NkuOiIiISC5Zl4I/++yzWLduHW644QbMmjULvXr1AgAcOXIEqamp0Ov1+Oc//6lIokRERESWkFXcBAUFYefOnZg5cybmz58PIQQAQJIkxMfHIzU1FUFBQYokSkRERGQJ2TfxCwsLw4YNG/Drr7/i2LFjEEKgZ8+e6NixoxL5EREREcli1R2KAaBjx44YPHiwLXMhIiIiajWrni1FRERE5KhY3BAREZGmsLghIiIiTWFxQ0RERJrC4oaIiIg0hcUNERERaQqLGyIiItIUFjdERESkKVbfxI+IiGxLggHdnErh7VSNSoMHCg0BEDb+H5R9kBpJMCBUuohCC9uzuCEisrMOUjUmeuzBZM/t6OZcahx+Sh+A9y/fijXVQ1AlPNhHG/VBjuPq9e1fUwo/C6eTRMPTL9sJnU4HPz8/jOo4GS6Sm73TISI7y1/Yy679d3Eqw0d+aQh3vghAwEn6fZzht3fnk/rOeLj8MZw1+LMPhfsgx3Ht+q68DPjNAMrLy+Hr69vstA5xDC81NRXh4eHw8PBATEwM9uzZ02z7tWvX4sYbb4SHhwf69u2LDRs2tFGmRES200Gqxkd+aejmXAonyfTDGgCcpPpXN+dSfOSXhg5SNftQsA9yHC2t75bYvbjJzMxEYmIikpKSkJubi/79+yM+Ph7nzp0z237nzp148MEHMW3aNOzbtw/jx4/H+PHjceDAgTbOnIiodSZ47EG480W4SIZm27lIBoQ7X8CfPL5nHwr2QY7D0vXdFLsXN0uWLMGMGTMwdepU9O7dG2lpafDy8kJ6errZ9suXL8fo0aPx9NNPIyoqCsnJyRg0aBDeeOONNs6ciMh6EgyY4rkdgOVnBkzx/A4SLH+zZx/WfTCSfVmzvq9l1+KmtrYWe/fuRVxcnHGYk5MT4uLikJOTY3aanJwck/YAEB8f32T7mpoa6HQ6kxcRkb11cyr97ZC7Ze2dJCDMuRTdnEpbbsw+ZPdBjkPu+jbHrldLXbhwAXq9HkFBQSbDg4KCcOTIEbPTFBcXm21fXFxstn1KSgoWLVrUaPi6kytaPCGJiEgxpbnAxpdlT5b90J+AgEHsw9Z9kOOwcn1fze5fSylt/vz5KC8vN76KiorsnRIREeBq5T9XcqZjH9ZNR/Zlg/Vm1+Kmc+fOcHZ2RklJicnwkpISBAcHm50mODhYVnt3d3f4+vqavIiI7M67O9AhAoClx96l+mm8u7MPJfogxyF7fTdm1+LGzc0N0dHRyMrKMg4zGAzIyspCbGys2WliY2NN2gPApk2bmmxPROSQJCeg12x509wwu3469mH7PshxWLO+r2H3NZ+YmIiVK1fi/fffx+HDhzFz5kxUVVVh6tSpAICEhATMnz/f2P7JJ5/Exo0b8dprr+HIkSN47rnn8MMPP2DWrFn2mgUiIutETgN8egBSC6c/Si6AT08g8lH2oWQf5DgsXd9NsHtxM3HiRLz66qtYuHAhBgwYgLy8PGzcuNF40nBhYSHOnj1rbD9s2DB88skneOedd9C/f3/85z//wfr163HTTTfZaxaIiKzj6gOMzAK8Gw7BX3sY/rdh3t2BkZvr27MP5fogx9Hi+m5eu338giW3byYiahNXKoDj7wH5K4Cqgt+He3ev/3ol8tHWf1izD1Kjq9a37nyBxY9fYHFDROQohAGoPAFc0dVfMeLd3fbnjbAPUiNhgO7Mj/DrOsiiz+9291TwhlqON/MjIscUCLgE1t+ctaKSfdi9D3IUOnEdgN8/x5vT7oqbixcvAgBCQ0PtnAkRERHJVVFRAT8/v2bbtLviJiAgAED9icotLRy10+l0CA0NRVFRkea/guO8alN7mlegfc0v51WblJxXIQQqKioQEhLSYtt2V9w4OdV/J+vn56f5jaxBe7p5IedVm9rTvALta345r9qk1LxaelCCZ18RERGRprC4ISIiIk1pd8WNu7s7kpKS4O7ubu9UFMd51SbOq3a1p/nlvGqTo8xru7vPDREREWlbuztyQ0RERNrG4oaIiIg0hcUNERERaQqLGyIiItIUFjdERESkKSxuiIiISFNY3BAREZGmsLghIiIiTWFxQ0RERJrC4oaIiIg0hcUNERERaQqLGyIiItIUFjdERESkKSxuiIiISFNc7J1AWzMYDDhz5gx8fHwgSZK90yEiIiILCCFQUVGBkJAQODk1f2ym3RU3Z86cQWhoqL3TICIiIisUFRWha9euzbZpd8WNj48PgPqF4+vra+dsiIiIyBI6nQ6hoaHGz/HmtLvipuGrKF9fXxY3REREKmPJKSU8oZiIiIg0hcUNERERaYpdi5tt27bh3nvvRUhICCRJwvr161ucJjs7G4MGDYK7uzt69OiBjIwMxfMkIiIi9bDrOTdVVVXo378/Hn30UfzhD39osX1BQQHGjh2Lxx57DB9//DGysrIwffp0dOnSBfHx8W2QsbKOHj2K9PR0nDx5EuHh4Xj00UfRs2dPm03P+MrGVzu1L3+1xyf7Uvv2o/b4NiccBADx2WefNdvmmWeeEX369DEZNnHiRBEfH29xP+Xl5QKAKC8vtyZNxaSnpwsnJyfh7Oxs8nPVqlU2mZ7xlY2vdmpf/mqPT/al9u1H7fEtJefzWxJCCPuVVr+TJAmfffYZxo8f32Sb22+/HYMGDcKyZcuMw1atWoU5c+agvLzc7DQ1NTWoqakx/t1wKVm3l1+Ak4eHrdJvlSvnz+P0S4sBc6tCknD9//0Drtd1tnr6oMdmoCRtJeMrFF/t1L781R6f7Evt24/a48thqK5G4bxnUV5e3uLVzqo6obi4uBhBQUEmw4KCgqDT6XD58mWz06SkpMDPz8/4csQb+FXu+h5o6tI2SULlrj2tmr5sw9eMr2B8tVP78ld7fLIvtW8/ao+vFFUVN9aYP38+ysvLja+ioiJ7p9RIXWmp+aoXAISoH9+a6cvLGV/B+Gqn9uWv9vhkX2rfftQeXymqKm6Cg4NRUlJiMqykpAS+vr7w9PQ0O427u7vxhn2OeuM+l4CAZitfl4CA1k3v58f4CsZXO7Uvf7XHJ/tS+/aj9vhKUVVxExsbi6ysLJNhmzZtQmxsrJ0ysg3voYObrXy9hw5p1fT+Y+IZX8H4aqf25a/2+GRfat9+1B5fKXYtbiorK5GXl4e8vDwA9Zd65+XlobCwEED9V0oJCQnG9o899hhOnDiBZ555BkeOHMGbb76JNWvWYO7cufZI32Zcr7sOnSZNqK9+nZxMfnaaNKHFk7Famt6z1w2Mr2B8tVP78ld7fLIvtW8/ao+vFIuulvrpp58sDtivXz+L22ZnZ2PEiBGNhk+ePBkZGRmYMmUKTp48iezsbJNp5s6di0OHDqFr165YsGABpkyZYnGfOp0Ofn5+DnW1VIMr5y+gctce1JWWwiUgAN5Dh8jaMFqanvGVja92al/+ao9P9qX27Uft8S0h52opi4obJycnSJKEppo2jJMkCXq93rqs24gjFzdERERknpzixqI7FBcUFNgkMSIiIiKlWVTchIWFKZ0HERERkU1YdULxhx9+iFtuuQUhISE4deoUAGDZsmX4/PPPbZocERERkVyyi5u33noLiYmJGDNmDMrKyozn2Pj7+5s8FoGIiIjIHmQXN6+//jpWrlyJf/7zn3B2djYOv/nmm7F//36bJkdEREQkl+zipqCgAAMHDmw03N3dHVVVVTZJioiIiMhasoubiIgI4033rrZx40ZERUXZIiciIiIiq1l0tdTVEhMT8fjjj6O6uhpCCOzZswf//ve/kZKSgnfffVeJHImIiIgsJru4mT59Ojw9PfHss8/i0qVL+POf/4yQkBAsX74ckyZNUiJHIiIiIovJLm4A4KGHHsJDDz2ES5cuobKyEoGBgbbOi4iIiMgqVhU3Dby8vODl5WWrXIiIiIhazaLiZuDAgZAkyaKAubm5rUqIiIiIqDUsKm7Gjx9v/L26uhpvvvkmevfujdjYWADArl27cPDgQfztb39TJEkiIiIiS1lU3CQlJRl/nz59OmbPno3k5ORGbYqKimybHREREZFMsu9zs3btWiQkJDQa/vDDD+PTTz+1SVJERERE1pJd3Hh6emLHjh2Nhu/YsQMeHh42SYqIiIjIWrKvlpozZw5mzpyJ3NxcDBkyBACwe/dupKenY8GCBTZPkIiIiEgO2cXNvHnz0L17dyxfvhwfffQRACAqKgqrVq3ChAkTbJ4gERERkRxW3edmwoQJLGSIiIjIIVl9E7+9e/fi8OHDAIA+ffqYfVI4ERERUVuTXdycO3cOkyZNQnZ2Nvz9/QEAZWVlGDFiBFavXo3rrrvO1jkSERERWUz21VJPPPEEKioqcPDgQZSWlqK0tBQHDhyATqfD7NmzlciRiIiIyGKyj9xs3LgRmzdvRlRUlHFY7969kZqairvuusumyRERERHJJfvIjcFggKura6Phrq6uMBgMNkmKiIiIyFqyi5uRI0fiySefxJkzZ4zDTp8+jblz52LUqFE2TY6IiIhILtnFzRtvvAGdTofw8HBERkYiMjISERER0Ol0eP3115XIkYiIiMhiss+5CQ0NRW5uLjZv3owjR44AqL+JX1xcnM2TIyIiIpLLqvvcSJKEO++8E3feeaet8yEiIiJqFauKm++//x5btmzBuXPnGp1EvGTJEpskRkRERGQN2cXNSy+9hGeffRa9evVCUFAQJEkyjrv6dyIiIiJ7kF3cLF++HOnp6ZgyZYoC6RARERG1juyrpZycnHDLLbcokQsRERFRq8kububOnYvU1FQlciEiIiJqNdlfSz311FMYO3YsIiMj0bt370Z3K163bp3NkiMiIiKSS3ZxM3v2bGzZsgUjRoxAp06dbHIScWpqKl555RUUFxejf//+eP311zFkyBCzbTMyMjB16lSTYe7u7qiurm51Hq115fx5VO76HnWlpXAJCID30MFwveop6S2Nb218pfNnfMZnfMZnfMZ3xPjXkoQQQs4EPj4+WL16NcaOHWuTBDIzM5GQkIC0tDTExMRg2bJlWLt2LfLz8xEYGNiofUZGBp588knk5+cbh0mShKCgIIv60+l08PPzQ7eXX4CTh4dN5gEAKnbvwcXVawFJAoQw/uw0aQJ8Yga3OL618ZXOn/EZn/EZn/EZ357xDdXVKJz3LMrLy+Hr69tsW9nn3AQEBCAyMlLuZE1asmQJZsyYgalTp6J3795IS0uDl5cX0tPTm5xGkiQEBwcbX5YWNkq5cv58/YoTAjAYTH5eXL0Gl/N/bnb8lfMXWhW/pelbmz/jMz7jMz7jM74jxm+K7OLmueeeQ1JSEi5dutTqzmtra7F3716TRzc4OTkhLi4OOTk5TU5XWVmJsLAwhIaGYty4cTh48GCTbWtqaqDT6Uxetla56/v6StQcSULZhq+bHV+5a0+r4rc0fUsYn/EZn/EZn/HVGL8pss+5WbFiBY4fP46goCCEh4c3OqE4NzfX4lgXLlyAXq9vdOQlKCjI+Nyqa/Xq1Qvp6eno168fysvL8eqrr2LYsGE4ePAgunbt2qh9SkoKFi1aZHFO1qgrLa2vRs0RAnXl5c2PLy1tXfwWpm8J4zM+4zM+4zO+GuM3RXZxM378eAXSsFxsbCxiY2ONfw8bNgxRUVF4++23kZyc3Kj9/PnzkZiYaPxbp9MhNDTUpjm5BAT8/l3itSQJLn5+0Ot0TY8PCGhd/BambwnjMz7jMz7jM74a4zdF9tdSSUlJzb7k6Ny5M5ydnVFSUmIyvKSkBMHBwRbFcHV1xcCBA3Hs2DGz493d3eHr62vysjXvoYObrUz9x8Q3O957qPkrwyyN39L0LWF8xmd8xmd8xldj/KbILm5syc3NDdHR0cjKyjIOMxgMyMrKMjk60xy9Xo/9+/ejS5cuSqXZItfrrkOnSRPqq1MnJ5OfnSZNgGevG5od73pd51bFb2n61ubP+IzP+IzP+IzviPGbIvtScFvLzMzE5MmT8fbbb2PIkCFYtmwZ1qxZgyNHjiAoKAgJCQm4/vrrkZKSAgB4/vnnMXToUPTo0QNlZWV45ZVXsH79euzduxe9e/dusT+lLgUHgCvnL6By156rruMfYrLiWhrf2vhK58/4jM/4jM/4jG+v+HIuBbd7cQMAb7zxhvEmfgMGDMCKFSsQExMDALjjjjsQHh6OjIwMAPWPf1i3bh2Ki4vRsWNHREdH44UXXsDAgQMt6kvJ4oaIiIiUobripi2xuCEiIlIfRW/i16C2thb5+fmoq6uzNgQRERGRzckubi5duoRp06bBy8sLffr0QWFhIQDgiSeewMsvv2zzBImIiIjkkF3czJ8/Hz/++COys7PhcdXXOnFxccjMzLRpckRERERyyb6J3/r165GZmYmhQ4fi6ieC9+nTB8ePH7dpckRERERyyT5yc/78ebNP666qqjIpdoiIiIjsQXZxc/PNN+Orr74y/t1Q0Lz77rsW33iPiIiISCmyv5Z66aWXcPfdd+PQoUOoq6vD8uXLcejQIezcuRNbt25VIkciIiIii8k+cnPrrbciLy8PdXV16Nu3L7755hsEBgYiJycH0dHRSuRIREREZDHZR24AIDIyEitXrrR1LkREREStJvvIjbOzM86dO9do+MWLF+Hs7GyTpIiIiIisJbu4aeppDTU1NXBzc2t1QkREREStYfHXUitWrABQf3XUu+++C29vb+M4vV6Pbdu24cYbb7R9hkREREQyWFzcLF26FED9kZu0tDSTr6Dc3NwQHh6OtLQ022dIREREJIPFxU1BQQEAYMSIEVi3bh06duyoWFJERERE1pJ9tdSWLVuUyIOIiIjIJqy6FPyXX37Bf//7XxQWFqK2ttZk3JIlS2ySGBEREZE1ZBc3WVlZuO+++9C9e3ccOXIEN910E06ePAkhBAYNGqREjkREREQWk30p+Pz58/HUU09h//798PDwwKeffoqioiIMHz4cf/rTn5TIkYiIiMhisoubw4cPIyEhAQDg4uKCy5cvw9vbG88//zz+9a9/2TxBIiIiIjlkFzcdOnQwnmfTpUsXHD9+3DjuwoULtsuMiIiIyAqyz7kZOnQotm/fjqioKIwZMwZ///vfsX//fqxbtw5Dhw5VIkciIiIii8kubpYsWYLKykoAwKJFi1BZWYnMzEz07NmTV0oRERGR3ckubrp37278vUOHDrwrMRERETkU2efcdO/eHRcvXmw0vKyszKTwISIiIrIH2cXNyZMnodfrGw2vqanB6dOnbZIUERERkbUs/lrqv//9r/H3r7/+Gn5+fsa/9Xo9srKyEB4ebtPkiIiIiOSyuLgZP348AECSJEyePNlknKurK8LDw/Haa6/ZNDkiIiIiuSwubgwGAwAgIiIC33//PTp37qxYUkRERETWkn21VEFBgRJ5EBEREdmExScU5+Tk4MsvvzQZ9sEHHyAiIgKBgYH4y1/+gpqaGpsnSERERCSHxcXN888/j4MHDxr/3r9/P6ZNm4a4uDjMmzcPX3zxBVJSUhRJkoiIiMhSFhc3eXl5GDVqlPHv1atXIyYmBitXrkRiYiJWrFiBNWvWKJIkERERkaUsPufm119/RVBQkPHvrVu34u677zb+PXjwYBQVFdk2OyIVuXL+PCp3fY+60lK4BATAe+hguF53ncXj7R2f7Evt24/a45N9WbJ+K7bnWBxPEkIISxqGhYXhww8/xO23347a2lr4+/vjiy++MB7N2b9/P4YPH47S0lKZs9S2dDod/Pz80O3lF+Dk4WHvdEgjKnbvwcXVawFJAoQw/uw0aQJ8Yga3ON7e8cm+1L79qD0+2ZfF6xcAhEB5eTl8fX2bjWnx11JjxozBvHnz8N1332H+/Pnw8vLCbbfdZhz/008/ITIy0ro5I1KxK+fP1+94QgAGg8nPi6vX4HL+z82Ov3L+gl3jk32pfftRe3yyL1nr17JjMQBkFDfJyclwcXHB8OHDsXLlSqxcuRJubm7G8enp6bjrrrvkzdVvUlNTER4eDg8PD8TExGDPnj3Ntl+7di1uvPFGeHh4oG/fvtiwYYNV/RLZQuWu7+v/0zBHklC24etmx1fuan57Vzo+2Zfatx+1xyf7atX6bYbF59x07twZ27ZtQ3l5Oby9veHs7Gwyfu3atfD29padQGZmJhITE5GWloaYmBgsW7YM8fHxyM/PR2BgYKP2O3fuxIMPPoiUlBTcc889+OSTTzB+/Hjk5ubipptukt0/UWvVlZY2/R+FEKgrL29+fAtf5Sodn+xL7duP2uOTfbVq/TZD9oMz/fz8GhU2ABAQEGByJMdSS5YswYwZMzB16lT07t0baWlp8PLyQnp6utn2y5cvx+jRo/H0008jKioKycnJGDRoEN544w2z7WtqaqDT6UxeRLbkEhDQ7H8eLn5+zY8PCLBrfLIvtW8/ao9P9tWq9dscYUc1NTXC2dlZfPbZZybDExISxH333Wd2mtDQULF06VKTYQsXLhT9+vUz2z4pKUkAaPQqLy+3xSwQiZ9//lk4OTmZ3c6cnJzEpk2bmh1/9OhRu8Yn+1L79qP2+GRf1qxfSz6/7VrcnD59WgAQO3fuNBn+9NNPiyFDhpidxtXVVXzyyScmw1JTU0VgYKDZ9tXV1aK8vNz4KioqYnFDNrdq1Srh5OQknJ2dTX6uWrXKovH2jk/2pfbtR+3xyb4sXb8NRY4ln98WXwquhDNnzuD666/Hzp07ERsbaxz+zDPPYOvWrdi9e3ejadzc3PD+++/jwQcfNA578803sWjRIpSUlLTYZ8Ol4JZcSkYkx7Fjx/Dee+/h5MmTCA8Px7Rp09CjRw+Lx9s7PtmX2rcftccn+7Jk/b755ptYunSpRZ/fdi1uamtr4eXlhf/85z8YP368cfjkyZNRVlaGzz//vNE03bp1Q2JiIubMmWMclpSUhPXr1+PHH39ssU8WN0REROoj5/Nb9gnFtuTm5obo6GhkZWUZhxkMBmRlZZkcyblabGysSXsA2LRpU5PtiYiIqH2x+FJwpSQmJmLy5Mm4+eabMWTIECxbtgxVVVWYOnUqACAhIQHXX3+98aGcTz75JIYPH47XXnsNY8eOxerVq/HDDz/gnXfesedsEBERkYOwe3EzceJEnD9/HgsXLkRxcTEGDBiAjRs3Gp9jVVhYCCen3w8wDRs2DJ988gmeffZZ/N///R969uyJ9evX8x43REREBMDO59zYA8+5ISIiUh85n992P3LT1hpqOd7Mj4iISD0aPrctOSbT7oqbixcvAgBCQ0PtnAkRERHJVVFRAT8/v2bbtLviJuC3W3EXFha2uHDUTqfTITQ0FEVFRZr/Co7zqk3taV6B9jW/nFdtUnJehRCoqKhASEhIi23bXXHTcHKyn5+f5jeyBr6+vpxXDeK8ald7ml/OqzYpNa+WHpSw631uiIiIiGyNxQ0RERFpSrsrbtzd3ZGUlAR3d3d7p6I4zqs2cV61qz3NL+dVmxxlXtvdfW6IiIhI29rdkRsiIiLSNhY3REREpCksboiIiEhTWNwQERGRprC4ISIiIk1hcUNERESawuKGiIiINIXFDREREWkKixsiIiLSFBY3REREpCksboiIiEhTWNwQERGRprC4ISIiIk1hcUNERESa4mLvBNqawWDAmTNn4OPjA0mS7J0OERERWUAIgYqKCoSEhMDJqfljM+2uuDlz5gxCQ0PtnQYRERFZoaioCF27dm22Tbsrbnx8fAAA+fn5CA4OtmnsqqoqhISEAKgvojp06KCa+Mxdm/GZuzbjqzl3peMzd+3G1+l0CA0NNX6ON0d2cVNSUoKnnnoKWVlZOHfuHIQQJuP1er3ckG2q4asoHx8f+Pr62jS2s7Oz8XdfX1+br1iT+Pv3o8NddykTm7nbLz6XjfnYNl4uao+v5tyVjt+muSu5zavsvaYt4jew5JQS2ScUT5kyBbm5uViwYAH+85//YN26dSYvaiOLFtk7A+upOXelcdmYp/RyUXN8NeeudHzuT01T83q1gOziZvv27fj4448xc+ZMjB8/HuPGjTN5kYK2bfv99507gexsu6Uim5pzVxqXjXlKLxc1x1dz7krH5/7UNDWvV5lkFzehoaGNvoqy1rZt23DvvfciJCQEkiRh/fr1LU6TnZ2NQYMGwd3dHT169EBGRoZNclGF5OTff3dyAhYutF8ucqk5d6Vx2Zin9HJRc3w15650fO5PTVPzepVJdnGzbNkyzJs3DydPnmx151VVVejfvz9SU1Mtal9QUICxY8dixIgRyMvLw5w5czB9+nR8/fXXrc7F4WVnAzk5v/9tMADffaeO/0rUnLvSuGzMU3q5qDm+mnNXOj73p6apeb1aQ8jk7+8v3NzchJOTk/D29hYdO3Y0eVkLgPjss8+abfPMM8+IPn36mAybOHGiiI+Pt7if8vJyAUCcOXPGmjSbVVlZKQAIAKKystK2wW+9VVQ6Of0eHxDC2VmI226zSXjmbqf4XDbmKbxcVB1fzbkrHV/p3IWy+5Sa32vaYtk3fH6Xl5e32Fb21VLLli2TO4nN5OTkIC4uzmRYfHw85syZ0+Q0NTU1qKmpMf6t0+mUSk852dnA9u2Nh+v1v1fGd9zRxklZSM25K43Lxjyll4ua46s5d6Xjc39qmprXq5VkFzeTJ09WIg+LFBcXIygoyGRYUFAQdDodLl++DE9Pz0bTpKSkYJGZs7Yn3rsU7m4tXysvh15fa/z9lpkr4OzqbpO47371BvpKEmDmXKc6ScJPkx7FjLGzWtWH/srvBSBzb5v4XDbmKb1c1BxfzbkrHb8t9idA2X1Kre81bbbsa6stbmvVs6X0ej0+/fRTvPDCC3jhhRfw2WefOez9bebPn4/y8nLjq6ioyN4pyRJ99hgGlhTApYmTuF2EwKCSAkSfPdbGmbVMzbkrjcvGPKWXi5rjqzl3peNzf2qamtdra8gubo4dO4aoqCgkJCQY723z8MMPo0+fPjh+/LgSORoFBwejpKTEZFhJSQl8fX3NHrUBAHd3d/j6+pq81GRm7kbUtXDDojpJwmO5G9soI8upOXelcdmYp/RyUXN8NeeudHzuT01T83ptDdnFzezZsxEZGYmioiLk5uYiNzcXhYWFiIiIwOzZs5XI0Sg2NhZZWVkmwzZt2oTY2FhF+7WXliriBo74X4mac1cal415Si8XNcdXc+5Kx+f+1DQ1r9fWkl3cbN26FYsXL0ZAQIBxWKdOnfDyyy9j69atsmJVVlYiLy8PeXl5AOov9c7Ly0NhYSGA+q+UEhISjO0fe+wxnDhxAs888wyOHDmCN998E2vWrMHcuXPlzoYqWFIRN3C0/0rUnLvSuGzMU3q5qDm+mnNXOj73p6apeb22luzixt3dHRUVFY2GV1ZWws3NTVasH374AQMHDsTAgQMBAImJiRg4cCAW/nbjn7NnzxoLHQCIiIjAV199hU2bNqF///547bXX8O677yI+Pl7ubDg8SyviBo70X4mac1cal415Si8XNcdXc+5Kx+f+1DQ1r1dbkF3c3HPPPfjLX/6C3bt3QwgBIQR27dqFxx57DPfdd5+sWHfccYcxxtWvhrsOZ2RkIPuaGwDdcccd2LdvH2pqanD8+HFMmTJF7iyowszcjTDAsoq4gQGO8V+JmnNXGpeNeUovFzXHV3PuSsfn/tQ0Na9XW5Bd3KxYsQKRkZGIjY2Fh4cHPDw8cMstt6BHjx5Yvny5Ejm2Ox5XanDTuVNwgrzHXDhBoO+5U/Coq225sULUnLvSuGzMU3q5qDm+mnNXOj73p6apeb3aiuz73Pj7++Pzzz/H0aNHceTIEQBAVFQUevToYfPk2qtqV3eMnpQE7yum1/TX1dUC618DAEwY/3e4uDT+GrDS1QPVZoa3FTXnrjQuG/OUXi5qjq/m3JWOz/2paWper7Yiu7hp0LNnT/Ts2dOWudBVyjy9UebpbTLs6hs8nfbpZPMbstmKmnNXGpeNeUovFzXHV3PuSsfn/tQ0Na9XW7CouElMTERycjI6dOiAxMTEZtsuWbLEJokRERERWcOi4mbfvn24cuWK8XciIiIiR2VRcbNlyxazvxMRERE5GtlXSz366KNm73NTVVWFRx991CZJEREREVlLdnHz/vvv4/Lly42GX758GR988IFNkiIiIiKylsVXS+l0OuNN9ioqKuDh4WEcp9frsWHDBgQGBiqSJBEREZGlLC5u/P39IUkSJEnCDTfc0Gi8JElYtGiRTZMjIiIiksvi4mbLli0QQmDkyJH49NNPTR6c6ebmhrCwMISEhCiSJBEREZGlLC5uhg8fDqD+yd3dunWDZOGTQImIiIjakuwTir/99lv85z//aTR87dq1eP/9922SFBEREZG1ZBc3KSkp6Ny5c6PhgYGBeOmll2ySFBEREZG1ZBc3hYWFiIiIaDQ8LCwMhYWFNkmKiIiIyFqyi5vAwED89NNPjYb/+OOP6NSpk02SIiIiIrKW7OLmwQcfxOzZs7Flyxbo9Xro9Xp8++23ePLJJzFp0iQlciQiIiKymMVXSzVITk7GyZMnMWrUKLi41E9uMBiQkJDAc26IiIjI7mQXN25ubsjMzERycjJ+/PFHeHp6om/fvggLC1MiPyIiIiJZZBc3DcLDwyGEQGRkpPEIDhEREZG9yT7n5tKlS5g2bRq8vLzQp08f4xVSTzzxBF5++WWbJ0hEREQkh+ziZv78+fjxxx+RnZ1t8vDMuLg4ZGZm2jQ5IiIiIrlkf5+0fv16ZGZmYujQoSaPYOjTpw+OHz9u0+SIiIiI5JJd3Jw/fx6BgYGNhldVVanqeVOF8Z5w8e1g05iGWhdgT/3vHc7r4exSZ9P4+jq98Xdbx1cyttLx1Zy70vGZuzbjqzl3peMzd+3Gr7tieTzZX0vdfPPN+Oqrr4x/NxQ07777LmJjY+WGIyIiIrIp2UduXnrpJdx99904dOgQ6urqsHz5chw6dAg7d+7E1q1blciRiIiIyGKyj9zceuutyMvLQ11dHfr27YtvvvkGgYGByMnJQXR0tBI5EhEREVnMqhvUREZGYuXKlbbOhYiIiKjVLCpudDqdxQF9fX2tToaIiIiotSwqbvz9/Vu8EkoIAUmSoNfrm21HREREpCSLipstW7YonYfqGEQtnCQ3e6dBRERE17CouBk+fLjSeaiKXlTiZM0ieEhhCHAZDU/nHvZOiYiIiH4j+2opAPjuu+/w8MMPY9iwYTh9+jQA4MMPP8T27dttmpyjMohqAAZUi5M4c+UtnK55A5f1x+ydFhEREcGK4ubTTz9FfHw8PD09kZubi5qaGgBAeXk5XnrpJZsn6NgEABiLnDM1b9s5HyIiIpJd3LzwwgtIS0vDypUr4erqahx+yy23IDc316bJqUd9kVMjThmHlPucsFcyRERE7Zrs4iY/Px+33357o+F+fn4oKyuzKonU1FSEh4fDw8MDMTEx2LNnT5NtMzIyIEmSyevqp5M7ioM9V+HHXm+hzIcPEyUiImpLsoub4OBgHDvW+PyS7du3o3v37rITyMzMRGJiIpKSkpCbm4v+/fsjPj4e586da3IaX19fnD171vg6depUk23tSed9Cvt7vcMih4iIqA3JLm5mzJiBJ598Ert374YkSThz5gw+/vhjPPXUU5g5c6bsBJYsWYIZM2Zg6tSp6N27N9LS0uDl5YX09PQmp5EkCcHBwcZXUFBQk21ramqg0+lMXm1Gqv+66uoip9ybX1cREREpSXZxM2/ePPz5z3/GqFGjUFlZidtvvx3Tp0/HX//6VzzxxBOyYtXW1mLv3r2Ii4v7PSEnJ8TFxSEnJ6fJ6SorKxEWFobQ0FCMGzcOBw8ebLJtSkoK/Pz8jK/Q0FBZOdpEQ5HjcxIHeqZDL11p+xyIiIjaCdnFjSRJ+Oc//4nS0lIcOHAAu3btwvnz55GcnCy78wsXLkCv1zc68hIUFITi4mKz0/Tq1Qvp6en4/PPP8dFHH8FgMGDYsGH45ZdfzLafP38+ysvLja+ioiLZebaaqL+7s29FOG46Og3OwrWFCYiIiMhaVj04EwDc3NzQu3dv6HQ6bN68Gb169UJUVJQtczMrNjYWsbGxxr+HDRuGqKgovP3222YLLHd3d7i7uyuel1lCAiQB38owhJ25C/4VkfbJg4iIqB2RXdxMmDABt99+O2bNmoXLly9j8ODBKCgogBACq1evxgMPPGBxrM6dO8PZ2RklJSUmw0tKShAcHGxRDFdXVwwcONDsSc72xqKGiIio7cn+Wmrbtm247bbbAACfffYZDAYDysrKsGLFCrzwwguyYrm5uSE6OhpZWVnGYQaDAVlZWSZHZ5qj1+uxf/9+dOnSRVbfSutzdCr6589kYUNERNTGZBc35eXlCAgIAABs3LgRDzzwALy8vDB27FgcPXpUdgKJiYlYuXIl3n//fRw+fBgzZ85EVVUVpk6dCgBISEjA/Pnzje2ff/55fPPNNzhx4gRyc3Px8MMP49SpU5g+fbrsvm2n/pwadynMOMSvQv5l8URERNR6sr+WCg0NRU5ODgICArBx40asXr0aAPDrr79adTO9iRMn4vz581i4cCGKi4sxYMAAbNy40XiScWFhIZycfq/Bfv31V8yYMQPFxcXo2LEjoqOjsXPnTvTu3Vt2360nARDwkMIR4DIa7k6hKMD/2SEPIiIiaiC7uJkzZw4eeugheHt7IywsDHfccQeA+q+r+vbta1USs2bNwqxZs8yOy87ONvl76dKlWLp0qVX92IqT5AHAqdFTwQ36GrvmRURERFYUN3/7298QExODwsJC3HnnncajKt27d5d9zo1aOUveiHB/EU6Sm71TISIiomtYdSl4dHQ0oqOjTYaNHTvWJgmpBQsbIiIixyT7hGIiIiIiR8bihoiIiDSFxQ0RERFpCosbIiIi0hSrTiguKyvDnj17cO7cORgMBpNxCQkJNkmMiIiIyBqyi5svvvgCDz30ECorK+Hr6wtJkozjJElicUNERER2Jbu4+fvf/45HH30UL730Ery8vJTIqU10+/oy3N2sfii6WXp9LQp++73qOmc4u9o4/hW98Xdbx1cyttLxW4otCdvFv9TZfO5CajTIqvjtYb1auj4sWe6toeb4asrd3L7R3rZ5S0hCXevVLvFrLY8n+5yb06dPY/bs2aoubIiIiEi7ZBc38fHx+OGHH5TIhYiIiKjVZB8zGjt2LJ5++mkcOnQIffv2haurq8n4++67z2bJEREREcklu7iZMWMGAOD5559vNE6SJOj1+kbDiYiIiNqK7OLm2ku/iYiIiBwJb+JHREREmmJVcbN161bce++96NGjB3r06IH77rsP3333na1zIyIiIpJNdnHz0UcfIS4uDl5eXpg9ezZmz54NT09PjBo1Cp988okSORIRERFZTPY5Ny+++CIWL16MuXPnGofNnj0bS5YsQXJyMv785z/bNEEiIiIiOWQfuTlx4gTuvffeRsPvu+8+FBQUmJmCiIiIqO3ILm5CQ0ORlZXVaPjmzZsRGhpqk6SIiIiIrGXVs6Vmz56NvLw8DBs2DACwY8cOZGRkYPny5TZPkIiIiEgO2cXNzJkzERwcjNdeew1r1qwBAERFRSEzMxPjxo2zeYJEREREclj1yM77778f999/v61zISIiImo13sSPiIiINMWiIzcBAQH4+eef0blzZ3Ts2BGSJDXZtrS01GbJEREREcllUXGzdOlS+Pj4GH9vrrghIiIisieLipvJkycbf58yZYpSuRARERG1muxzbpydnXHu3LlGwy9evAhnZ2ebJEVERERkLdnFjRDC7PCamhq4ubm1OiEiIiKi1rD4UvAVK1YAACRJwrvvvgtvb2/jOL1ej23btuHGG2+0fYZEREREMlhc3CxduhRA/ZGbtLQ0k6+g3NzcEB4ejrS0NNtnqBH+lyvhfaVa9nSVrh4o8/RuuWErc6mrq8WPv/1+fcVFuLg0PgqnRC5yWZO7JIBKNw+UebScu391JbxrG8fPayY+AFS4qXPZAJavVzVvN0TWUHKbvza2JCx7r7H0vay9s7i4aXgo5ogRI7Bu3Tp07NhRsaS0xuNKDTauXgRXYZA97RXJCXc88iKqzWzktsylCkDD7rJm/Wvo0Aa5yNXq3B9+ETXN5O5xpQb/ayF+5ucaXTYt5K7m7YbIGkpu85bEbva9poX3MrLinJstW7awsJGp2tUdBwLDYIC8S+gNkLA/MMymHwqOlItcrc29pTeDald3HLiufS6blnJX83ZDZA0lt3ml38vIiuLmgQcewL/+9a9GwxcvXow//elPNklKi94aNBpOMH8ydlOcIJA2aLSmc5HL2tzfHmhZ7mntcNlYmruatxsiayi5zSv9XtbeyS5utm3bhjFjxjQafvfdd2Pbtm1WJZGamorw8HB4eHggJiYGe/bsabb92rVrceONN8LDwwN9+/bFhg0brOq3Le3t0gP7giJQZ+ENEOskCblBEdjbpYemc5FL6dz3dumBfYFcNvaIT+RolNzmuT8pS3ZxU1lZafaSb1dXV+h0OtkJZGZmIjExEUlJScjNzUX//v0RHx9v9l46ALBz5048+OCDmDZtGvbt24fx48dj/PjxOHDggOy+29pbg0bDpYlL6a/lIpT9j9eRcpFLbu5y/9NJUzi+kpRer2reboisoeQ2r/R7WXsmu7jp27cvMjMzGw1fvXo1evfuLTuBJUuWYMaMGZg6dSp69+6NtLQ0eHl5IT093Wz75cuXY/To0Xj66acRFRWF5ORkDBo0CG+88YbsvtuapZV6W1TojpSLXErnbunRm3a7bFS63RBZQ8ltnvuTcmQXNwsWLEBycjImT56M999/H++//z4SEhLw4osvYsGCBbJi1dbWYu/evYiLi/s9IScnxMXFIScnx+w0OTk5Ju0BID4+vsn2NTU10Ol0Ji97sqRSb6v/eB0pF7mUzt2SozeO+p+U0stGzdsNkTWU3OYtje2I7zUOTVjhyy+/FMOGDRNeXl6iU6dOYsSIESI7O1t2nNOnTwsAYufOnSbDn376aTFkyBCz07i6uopPPvnEZFhqaqoIDAw02z4pKUkAaPQ6c+aM7HxbUllZaYxfWVnZdMNbbxXC2VkIoPHL2VmI225rXXw5fsul8qplU2lBLnKpMncuG/vFFwotF43EV3PuSsdXLLaS27ya32vaMH55ebkAIMrLy1tsK/vIDQCMHTsWO3bsQFVVFS5cuIBvv/0Ww4cPd8jzXubPn4/y8nLjq6ioyN4pAcnJgF5vfpxeDzz/fPvMRS6lc+eysV98Ikej5DbP/cnmrCpurlZRUYF33nkHQ4YMQf/+/WVN27lzZzg7O6OkpMRkeElJCYKDg81OExwcLKu9u7s7fH19TV52d8cdwK23Atc+aNTZGbjttvrxbZ2L0zWbgj1ykUvp3Lls7BefyNEouc1zf7I5q4ubbdu2ISEhAV26dMGrr76KkSNHYteuXbJiuLm5ITo6GllZWcZhBoMBWVlZiI2NNTtNbGysSXsA2LRpU5PtHZa5St1eFXpyMmC45u7JavlvQencuWzsF5/I0Si5zXN/si0533edPXtWpKSkiB49eojAwEAxa9Ys4eLiIg4ePGj1d2irV68W7u7uIiMjQxw6dEj85S9/Ef7+/qK4uFgIIcQjjzwi5s2bZ2y/Y8cO4eLiIl599VVx+PBhkZSUJFxdXcX+/fst6q/hOzu7nnPT4Opzbyz4XlXR76ljY3+P7eRks+94jfHVnDuXjV3iq/38AFWeV6KB+IrnruQ2r+L3mraIL+ecG4uLm3vuuUf4+vqKBx98UHz55Zeirq5OCCFaXdwIIcTrr78uunXrJtzc3MSQIUPErl27jOOGDx8uJk+ebNJ+zZo14oYbbhBubm6iT58+4quvvrK4L4cqbrZsMT2ZeMsW28aXoXLDBtMT2VrIRXZ8NefOZWOX+Gp/I1b1B7iK4yueu5LbvIrfa9oiviLFjbOzs5g7d674+eefTYbborhpSw5V3AhRf/QGsKhCb7M3hGHDbBq7UXw1585l02bx1f5GrOoPcBXHb9PcldzmVfZe0xbxFblaavv27aioqEB0dDRiYmLwxhtv4MKFC5ZOTk158UXAy6v+p6NISrJ3BtZTOncuG/vFJ3I0Sm7z3J9axcXShkOHDsXQoUOxbNkyZGZmIj09HYmJiTAYDNi0aRNCQ0Ph4+OjZK42IX67WVJFRQU6dDD3QHnrVVVVGX/X6XTQN3Vp39UGDABOnAA8PYEWbjBoVXwLmcTu2xd6G9/sUDO5c9m0WXwll4va46s5d6Xjt2nuSm7zKnuvaYv4DTfhFS3c9BAAJGFJqybk5+fjvffew4cffoiysjLceeed+O9//2ttuDZx4sQJREZG2jsNIiIiskJRURG6du3abJtWFTcN9Ho9vvjiC6Snpzt8cVNWVoaOHTuisLAQfn5+9k5HUTqdDqGhoSgqKnKM+/soiPOqTe1pXoH2Nb+cV21Scl6FEKioqEBISAicrr0n0DUs/lqqOc7Ozsanczu6hgXi5+en+Y2sgcPcvLANcF61qT3NK9C+5pfzqk1KzaulByVafYdiIiIiIkfC4oaIiIg0pd0VN+7u7khKSoK7u7u9U1Ec51WbOK/a1Z7ml/OqTY4yrzY5oZiIiIjIUbS7IzdERESkbSxuiIiISFNY3BAREZGmsLghIiIiTdFkcZOamorw8HB4eHggJiYGe/bsabb92rVrceONN8LDwwN9+/bFhg0b2ihT66WkpGDw4MHw8fFBYGAgxo8fj/z8/GanycjIgCRJJi8PD482yth6zz33XKO8b7zxxmanUeM6bRAeHt5ofiVJwuOPP262vZrW67Zt23DvvfciJCQEkiRh/fr1JuOFEFi4cCG6dOkCT09PxMXF4ejRoy3GlbvPt4Xm5vXKlSv4xz/+gb59+6JDhw4ICQlBQkICzpw502xMa/aFttDSep0yZUqjvEePHt1iXLWtVwBm911JkvDKK680GdMR16slnzHV1dV4/PHH0alTJ3h7e+OBBx5ASUlJs3Gt3cfl0lxxk5mZicTERCQlJSE3Nxf9+/dHfHw8zp07Z7b9zp078eCDD2LatGnYt2+f8U7LBw4caOPM5dm6dSsef/xx7Nq1C5s2bcKVK1dw1113mTy4zBxfX1+cPXvW+Dp16lQbZdw6ffr0Mcl7+/btTbZV6zpt8P3335vM66ZNmwAAf/rTn5qcRi3rtaqqCv3790dqaqrZ8YsXL8aKFSuQlpaG3bt3o0OHDoiPj0d1dXWTMeXu822luXm9dOkScnNzsWDBAuTm5mLdunXIz8/Hfffd12JcOftCW2lpvQLA6NGjTfL+97//3WxMNa5XACbzePbsWaSnp0OSJDzwwAPNxnW09WrJZ8zcuXPxxRdfYO3atdi6dSvOnDmDP/zhD83GtWYft4rQmCFDhojHH3/c+LderxchISEiJSXFbPsJEyaIsWPHmgyLiYkRf/3rXxXN09bOnTsnAIitW7c22WbVqlXCz8+v7ZKykaSkJNG/f3+L22tlnTZ48sknRWRkpDAYDGbHq3W9AhCfffaZ8W+DwSCCg4PFK6+8YhxWVlYm3N3dxb///e8m48jd5+3h2nk1Z8+ePQKAOHXqVJNt5O4L9mBuXidPnizGjRsnK45W1uu4cePEyJEjm22jhvV67WdMWVmZcHV1FWvXrjW2OXz4sAAgcnJyzMawdh+3hqaO3NTW1mLv3r2Ii4szDnNyckJcXBxycnLMTpOTk2PSHgDi4+ObbO+oysvLAQABAQHNtqusrERYWBhCQ0Mxbtw4HDx4sC3Sa7WjR48iJCQE3bt3x0MPPYTCwsIm22plnQL12/RHH32ERx99FJIkNdlOrev1agUFBSguLjZZd35+foiJiWly3Vmzzzuq8vJySJIEf3//ZtvJ2RccSXZ2NgIDA9GrVy/MnDkTFy9ebLKtVtZrSUkJvvrqK0ybNq3Fto6+Xq/9jNm7dy+uXLliso5uvPFGdOvWrcl1ZM0+bi1NFTcXLlyAXq9HUFCQyfCgoCAUFxebnaa4uFhWe0dkMBgwZ84c3HLLLbjpppuabNerVy+kp6fj888/x0cffQSDwYBhw4bhl19+acNs5YuJiUFGRgY2btyIt956CwUFBbjttttQUVFhtr0W1mmD9evXo6ysDFOmTGmyjVrX67Ua1o+cdWfNPu+Iqqur8Y9//AMPPvhgsw8blLsvOIrRo0fjgw8+QFZWFv71r39h69atuPvuu6HX682218p6ff/99+Hj49PiVzWOvl7NfcYUFxfDzc2tUTHe0udtQxtLp7GWTZ4KTvb1+OOP48CBAy1+RxsbG4vY2Fjj38OGDUNUVBTefvttJCcnK52m1e6++27j7/369UNMTAzCwsKwZs0ai/4jUrP33nsPd999N0JCQppso9b1SvWuXLmCCRMmQAiBt956q9m2at0XJk2aZPy9b9++6NevHyIjI5GdnY1Ro0bZMTNlpaen46GHHmrxBH9HX6+WfsY4Ek0duencuTOcnZ0bna1dUlKC4OBgs9MEBwfLau9oZs2ahS+//BJbtmxB165dZU3r6uqKgQMH4tixYwplpwx/f3/ccMMNTeat9nXa4NSpU9i8eTOmT58uazq1rteG9SNn3VmzzzuShsLm1KlT2LRpU7NHbcxpaV9wVN27d0fnzp2bzFvt6xUAvvvuO+Tn58vefwHHWq9NfcYEBwejtrYWZWVlJu1b+rxtaGPpNNbSVHHj5uaG6OhoZGVlGYcZDAZkZWWZ/Gd7tdjYWJP2ALBp06Ym2zsKIQRmzZqFzz77DN9++y0iIiJkx9Dr9di/fz+6dOmiQIbKqaysxPHjx5vMW63r9FqrVq1CYGAgxo4dK2s6ta7XiIgIBAcHm6w7nU6H3bt3N7nurNnnHUVDYXP06FFs3rwZnTp1kh2jpX3BUf3yyy+4ePFik3mreb02eO+99xAdHY3+/fvLntYR1mtLnzHR0dFwdXU1WUf5+fkoLCxsch1Zs4+3ZgY0ZfXq1cLd3V1kZGSIQ4cOib/85S/C399fFBcXCyGEeOSRR8S8efOM7Xfs2CFcXFzEq6++Kg4fPiySkpKEq6ur2L9/v71mwSIzZ84Ufn5+Ijs7W5w9e9b4unTpkrHNtfO6aNEi8fXXX4vjx4+LvXv3ikmTJgkPDw9x8OBBe8yCxf7+97+L7OxsUVBQIHbs2CHi4uJE586dxblz54QQ2lmnV9Pr9aJbt27iH//4R6Nxal6vFRUVYt++fWLfvn0CgFiyZInYt2+f8Qqhl19+Wfj7+4vPP/9c/PTTT2LcuHEiIiJCXL582Rhj5MiR4vXXXzf+3dI+by/NzWttba247777RNeuXUVeXp7JPlxTU2OMce28trQv2Etz81pRUSGeeuopkZOTIwoKCsTmzZvFoEGDRM+ePUV1dbUxhhbWa4Py8nLh5eUl3nrrLbMx1LBeLfmMeeyxx0S3bt3Et99+K3744QcRGxsrYmNjTeL06tVLrFu3zvi3Jfu4LWiuuBFCiNdff11069ZNuLm5iSFDhohdu3YZxw0fPlxMnjzZpP2aNWvEDTfcINzc3ESfPn3EV1991cYZywfA7GvVqlXGNtfO65w5c4zLJSgoSIwZM0bk5ua2ffIyTZw4UXTp0kW4ubmJ66+/XkycOFEcO3bMOF4r6/RqX3/9tQAg8vPzG41T83rdsmWL2e22YX4MBoNYsGCBCAoKEu7u7mLUqFGNlkFYWJhISkoyGdbcPm8vzc1rQUFBk/vwli1bjDGundeW9gV7aW5eL126JO666y5x3XXXCVdXVxEWFiZmzJjRqEjRwnpt8PbbbwtPT09RVlZmNoYa1qslnzGXL18Wf/vb30THjh2Fl5eXuP/++8XZs2cbxbl6Gkv2cVuQfuuciIiISBM0dc4NEREREYsbIiIi0hQWN0RERKQpLG6IiIhIU1jcEBERkaawuCEiIiJNYXFDREREmsLihoiIiDSFxQ0RqdZzzz2HAQMGyJpGkiSsX79ekXyIyDGwuCEihyBJUrOv5557rtE0Tz31VKOHpBIRudg7ASIiADh79qzx98zMTCxcuBD5+fnGYd7e3sbfhRDQ6/Xw9vY2GU5EBPDIDRE5iODgYOPLz88PkiQZ/z5y5Ah8fHzwv//9D9HR0XB3d8f27dsbfS31/fff484770Tnzp3h5+eH4cOHIzc3t8k+a2trMWvWLHTp0gUeHh4ICwtDSkpKG8wtESmJxQ0Rqca8efPw8ssv4/Dhw+jXr1+j8RUVFZg8eTK2b9+OXbt2oWfPnhgzZgwqKirMxluxYgX++9//Ys2aNcjPz8fHH3+M8PBwheeCiJTGr6WISDWef/553HnnnU2OHzlypMnf77zzDvz9/bF161bcc889jdoXFhaiZ8+euPXWWyFJEsLCwmyeMxG1PR65ISLVuPnmm5sdX1JSghkzZqBnz57w8/ODr68vKisrUVhYaLb9lClTkJeXh169emH27Nn45ptvlEibiNoYixsiUo0OHTo0O37y5MnIy8vD8uXLsXPnTuTl5aFTp06ora01237QoEEoKChAcnIyLl++jAkTJuCPf/yjEqkTURvi11JEpBk7duzAm2++iTFjxgAAioqKcOHChWan8fX1xcSJEzFx4kT88Y9/xOjRo1FaWoqAgIC2SJmIFMDihog0o2fPnvjwww9x8803Q6fT4emnn4anp2eT7ZcsWYIuXbpg4MCBcHJywtq1axEcHAx/f/+2S5qIbI5fSxGRZrz33nv49ddfMWjQIDzyyCOYPXs2AgMDm2zv4+ODxYsX4+abb8bgwYNx8uRJbNiwAU5OfGskUjNJCCHsnQQRERGRrfDfEyIiItIUFjdERESkKSxuiIiISFNY3BAREZGmsLghIiIiTWFxQ0RERJrC4oaIiIg0hcUNERERaQqLGyIiItIUFjdERESkKSxuiIiISFP+H0PWHMA1+uA9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# SIMULATE BEHAVIOUR AND PLOT (MATPLOTLIB)\n",
    "def example_tmaze_plot(N = 60,pinit = 0.5,\n",
    "                       Nswitch = 10,pinit2 = 0.5,\n",
    "                        pHA = 1.0,pWin = 0.98,la = 2, rs = 3,\n",
    "                        Thorizon = 2,initial_conf = 0.5,\n",
    "                        action_selection_temp=32,memory_loss=0.0):\n",
    "    \"\"\" Plotting tmaze simulation results on a matplotlib figure :\n",
    "    - N : how many trials to simulate\n",
    "    - pinit : Initial cheese side\n",
    "    - Nswitch : trial at which the paradigm will switch. If Nswitch > N, the paradigm will not change\n",
    "    - pinit2 : Cheese side after the eventual switch\n",
    "    - pHA : P of clue showing the cheese side\n",
    "    - pWin : P of reward if you get the cheese side\n",
    "    - la : loss aversion, how much the mouse want to avoid the trap\n",
    "    - rs : reward seeking, how much the mouse seeks after the cheese\n",
    "    - Thorizon : how long into the future does the mouse look when planning its next action\n",
    "    - initial_conf : how confident is the mouse about its belief in the clue\n",
    "    - action_selection_temp : the higher this parameter, the more reluctant the agent will be to try a-priori suboptimal actions\n",
    "    - memory_loss : how much previous information is forgotten when learning new information\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate our data and store the results in a few arrays\n",
    "    true_cheese_state,true_mouse_state,clue_observations,mouse_cheese_perception,mouse_actions,mouse_action_posterior,starting_cheese_position,perceived_starting_cheese_position = generate_data(N,pinit,pHA,pWin,la,rs,Thorizon,initial_conf,Nswitch,pinit2,action_selection_temp,memory_loss,True,verbose=False)\n",
    "    print(\"Simulations completed : drawing the figure !\")\n",
    "    fig,axes = plt.subplots(3,sharex=True)\n",
    "    \n",
    "\n",
    "    infer_model = np.zeros((N*3,))\n",
    "    for i in range(N):\n",
    "        infer_model[i*3:(i*3+3)] = mouse_cheese_perception[0,:,i]\n",
    "    img_act = mouse_action_posterior[1:,0,:]\n",
    "    img_act[[0, 1, 2],:] = img_act[[1, 0, 2],:]\n",
    "\n",
    "\n",
    "    markersize=100\n",
    "    axes[0].imshow(starting_cheese_position,aspect='auto',interpolation='nearest',extent=[0,N+1,0,1],vmin=0,vmax=1)\n",
    "    axes[0].scatter(np.linspace(1,N+1,N),1.0-true_cheese_state[0,:],color='orange',marker=\".\",s=3*markersize)\n",
    "    print(true_cheese_state[0,:])\n",
    "    axes[1].scatter(np.linspace(1,N+1,3*N),(infer_model),color='black',marker=\".\",s=markersize)\n",
    "    axes[2].axvline(0,color='black')\n",
    "\n",
    "    for trial in range(mouse_actions.shape[1]) :\n",
    "        axes[2].axvline(trial+1,color='black')\n",
    "        \n",
    "    for trial in range(mouse_actions.shape[1]) :\n",
    "        pos = 0\n",
    "        t = 0\n",
    "        acts = mouse_actions[:,trial]\n",
    "        for act in acts :\n",
    "            if (t==0): # First timestep :\n",
    "                if (act==1):\n",
    "                    axes[2].scatter([trial+1+pos],[0.5],color='green',marker='>',s=markersize)\n",
    "                elif (act==0):\n",
    "                    axes[2].scatter([trial+1+pos],[0.5],color='orange',marker='<',s=markersize)\n",
    "\n",
    "            if (act==2):\n",
    "                axes[2].scatter([trial+1+pos],[1],color='red',marker='^',s=markersize)\n",
    "                break\n",
    "            elif (act==3):\n",
    "                axes[2].scatter([trial+1+pos],[0],color='red',marker='v',s=markersize)\n",
    "                break\n",
    "            pos = pos + 0.5\n",
    "            t = t +1\n",
    "    \n",
    "    axes[2].imshow(img_act,aspect='auto',interpolation='nearest',extent=[0,N+1,0,1],vmin=0,vmax=1)\n",
    "    axes[1].imshow(perceived_starting_cheese_position,aspect='auto',interpolation='nearest',extent=[0,N+1,0,1],vmin=0,vmax=1)\n",
    "\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_ylim([-0.1,1.1])\n",
    "        ax.set_xlim([0,N+1])\n",
    "    axes[0].set_ylabel(\"Context\")\n",
    "    axes[1].set_ylabel(\"State model\")\n",
    "    axes[2].set_ylabel(\"Action selection\")\n",
    "    axes[2].set_xlabel(\"Trials\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "myfig = example_tmaze_plot(60,pinit=0.95,Nswitch=10,pinit2=0.5,pHA=0.85,action_selection_temp=32,memory_loss=0)\n",
    "myfig.show()\n",
    "# myfig.savefig(os.path.join(\"local_resources/tmaze/renders\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to read this ?\n",
    "\n",
    "![Tutorial read the plot](./local_resources/tmaze/read_the_plot.png)\n",
    "\n",
    "\n",
    "### A few cool plots !\n",
    "\n",
    "Remember the gifs we generated earlier ? These are plots with the same parameters !\n",
    "\n",
    "1. Reliable clue, random environment       | 2. Reliable clue , environment stabilizes after trial 10 | 3. Unreliable clue & random environment\n",
    ":--------------------------------------:|:------------------------------------:|:------------------------------------:\n",
    "![Image1](local_resources/tmaze/renders/render_good_clue.png) |![Image2](local_resources/tmaze/renders/render_good_clue_cheese_stabilizes_at_10.png)|![Image3](local_resources/tmaze/renders/render_bad_clue_random_env.png)\n",
    "$p_{init} = 0.5$ , $pHA = 1.0$|$p_{init} = 0.5$ for the first 10 trials, then $p_{init} = 0.05$ , $pHA = 1.0$|$p_{init} = 0.5$ , $pHA = 0.5$\n",
    "\n",
    "And here are a few other examples if we play with some of the other parameters !\n",
    "\n",
    "4. Brutal paradigm shift, no memory loss       | 5. Brutal paradigm shift, with memory loss | 6. A paradigm shift with noiser action selection\n",
    ":--------------------------------------:|:------------------------------------:|:------------------------------------:\n",
    "![Image1](local_resources/tmaze/plot_paradigm_shift.png)|![Image3](local_resources/tmaze/plot_paradigm_shift_memory_loss.png) |![Image2](local_resources/tmaze/plot_paradigm_shift_noisy_selection.png)\n",
    "$p_{init} = 0.0$ for the first 10 trials, then $p_{init} = 0.5$ , $pHA = 1.0$|Same as on the left, but with the memory loss parameter set to 10. |The action_selection_temp has been set to 5 instead of the usual 32, \n",
    "\n",
    "\n",
    "#### Parameters for the previous simulations :\n",
    "\n",
    "**action_selection_temp :** (the Active Inference $\\alpha$ parameter.) The higher its value, the more detemrinistic the agent will be when selecting an action from the perceived action poster, making it less likely that the agent will explore seemingly suboptimal solution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behavioural_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
