{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> active-pynference </b> : Sophisticated Inference with Jax !\n",
    "\n",
    "<b>actynf</b> implements Sophisticated Inference with jax functions ! This allows us to beneficiate from the Just-In-Time compilation, auto-vectorization and auto-differntiation abilities of the package. This notebook is used to compare the results of the [SPM12's implementation of sophisticated inference](https://github.com/spm/spm/blob/main/toolbox/DEM/spm_MDP_VB_XX.m), the *numpy* implementation of this package, and the *jax* implementation of this package.\n",
    "\n",
    "**Note :** Writing in Jax comes with a number of constraints that don't exist in classical Python (see [this page](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) for the common pitfalls). This means that jax_actynf does not do exactly the same operations as classical sophisticated inference implementations (tree pruning, dynamic variable-dependent conditionning, etc.). Depending on your goal, it may be more interesting to switch to a Jax-based model , or remain in a classical (numpy) based environment. We give a few details regarding this point by the end of this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 . Environment : simple T-maze\n",
    "\n",
    "We'll be using a close analog to the T-maze environment here. The [basics about the T-maze environment](Tmaze_demo.ipynb) remain the same, but clue and reward modalities are fused together. The MDP weights for this situation are available in [this](../actynf/demo_tools/tmaze/weights.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actynf version : 0.1.39\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'demos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mactynf\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActynf version : \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(actynf\u001b[38;5;241m.\u001b[39m__version__))\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdemos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocal_demo_tools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtmaze\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweights\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_T_maze_gen_process,get_T_maze_model,get_jax_T_maze_model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'demos'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax import vmap\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import actynf\n",
    "print(\"Actynf version : \" + str(actynf.__version__))\n",
    "from local_demo_tools.tmaze.weights import get_T_maze_gen_process,get_T_maze_model,get_jax_T_maze_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the other demo, the weights depend on scalar parameters describing the properties of the true environment as well as the initial model of the artificial mouse : \n",
    "\n",
    "For the environment (process) :\n",
    "- $p_{init}$ is the probability of the reward being on the right at the beginning.\n",
    "- $p_{HA}$ is the probability of the clue showing the right (resp. left) when the reward is on the right (resp.left).\n",
    "- $p_{win}$ is the probability of getting a positive (resp. adversive) stimulus when picking the reward (resp. the shock).\n",
    "\n",
    "For the mouse model : \n",
    "- $p_{HA}$ is the mouse belief about the mapping of the clue\n",
    "- *initial_hint_confidence* is the strenght of this belief\n",
    "- $la$,$rs$ are the agent priors about receiving adversive vs positive stimuli.\n",
    "- $p_{win}$ is the mouse belief about probability of getting a positive (resp. negative) stimulus when picking the reward (resp. the shock).\n",
    "- *context_belief* is where the mouse thinks the reward spawns at the beginning of each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3\n",
    "Th = 2\n",
    "\n",
    "# Those weights will remain the same for the whole notebook :\n",
    "true_process_pHA = 1.0\n",
    "true_process_pinit = 1.0\n",
    "true_process_pwin = 0.98  # For a bit of noise !\n",
    "\n",
    "true_A,true_B,true_D,U = get_T_maze_gen_process(true_process_pinit,true_process_pHA,true_process_pwin)\n",
    "\n",
    "\n",
    "true_model_pHA = 1.0\n",
    "true_model_pwin = 0.98\n",
    "true_model_context_belief = 0.5\n",
    "true_model_hint_conf = 2.0\n",
    "true_model_la = -4.0\n",
    "true_model_rs = 2.0\n",
    "true_alpha = 16.0\n",
    "\n",
    "true_a,true_b,true_c,true_d,true_e,_ = get_T_maze_model(true_model_pHA,true_model_pwin,true_model_hint_conf,\n",
    "                                        true_model_la,true_model_rs,true_model_context_belief)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simulating Active Inference agents trials with SPM, actynf_classic and actynf_jax\n",
    "\n",
    "We compare the outcomes of 3 implementation of the same sophisticated inference algorithm, with a few noteable differences : \n",
    "- The original Matlab script, using a recursive (+ tree pruning) approach\n",
    "- Our classical Python implementation\n",
    "- Our Jax-based implementation\n",
    "\n",
    "\n",
    "\n",
    "1. For the first results, we used modified versions of the original SPM files spm_MDP_VB_XX.m and DEM_demo_MDP_XX.m. These modified files are available [here](./SPM_XX/). You can run them yourself with the same options as in this notebook and compare the displayed results. \n",
    "For the values above, we get : \n",
    "\n",
    "```\n",
    ">> DEM_demo_MDP_XX\n",
    "Computed EFE for t = 1\n",
    "  -11.4325  -12.8459  -12.8459   -9.5125\n",
    "\n",
    "Computed EFE for t = 2\n",
    "   -6.5111  -10.3886   -4.6286   -6.5111\n",
    "\n",
    "Computed EFE for t = 3\n",
    "   -1.3863   -1.3863   -1.3863   -1.3863\n",
    "```\n",
    "\n",
    "Let's now make the same computation with the classical *actynf* SI paradigm, and then see the actynf_jax result !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annic\\OneDrive\\Bureau\\MainPhD\\code\\venvs\\jax_new\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 2. Classical actynf simulations : \n",
    "from actynf import layer,link,layer_network\n",
    "\n",
    "def get_tmaze_net_classic():\n",
    "    # The T-maze environment : \n",
    "    process_layer = layer(\"T-maze_environment\",\"process\",true_A,true_B,None,true_D,None,U,T)\n",
    "    \n",
    "    \n",
    "    # The mouse model :\n",
    "    model_layer = layer(\"mouse_model\",\"model\",true_a,true_b,true_c,true_d,true_e,U,T,T_horiz=Th)\n",
    "    # This time, we define our layer as a \"model\" \n",
    "\n",
    "    # Here, we give a few hyperparameters guiding the beahviour of our agent :\n",
    "    model_layer.hyperparams.alpha = true_alpha # action precision : \n",
    "        # for high values the mouse will always perform the action it perceives as optimal, with very little exploration \n",
    "        # towards actions with similar but slightly lower interest\n",
    "\n",
    "    model_layer.learn_options.eta = 1.0 # learning rate (shared by all channels : a,b,c,d,e)\n",
    "    model_layer.learn_options.learn_a = True  # The agent learns the reliability of the clue\n",
    "    model_layer.learn_options.learn_b = False # The agent does not learn transitions\n",
    "    model_layer.learn_options.learn_d = True  # The agent has to learn the initial position of the cheese\n",
    "    model_layer.learn_options.backwards_pass = True  # When learning, the agent will perform a backward pass, using its perception of \n",
    "                                               # states in later trials (e.g. I saw that the cheese was on the right at t=3)\n",
    "                                               # as well as what actions it performed (e.g. and I know that the cheese position has\n",
    "                                               # not changed between timesteps) to learn more reliable weights (therefore if my clue was\n",
    "                                               # a right arrow at time = 2, I should memorize that cheese on the right may correlate with\n",
    "                                               # right arrow in general)\n",
    "    model_layer.learn_options.memory_loss = 0.0\n",
    "                                            # How many trials will be needed to \"erase\" 50% of the information gathered during one trial\n",
    "                                            # Used during the learning phase. 0.0 means that the mouse doens't forget\n",
    "                                            # anything.\n",
    "\n",
    "    \n",
    "    \n",
    "    # Create a link from observations generated by the environment to the mouse sensory states :\n",
    "    model_layer.inputs.o = link(process_layer, lambda x : x.o)\n",
    "    #     the layer from which we get the data | the function extracting the data\n",
    "\n",
    "    # Create a link from the actions selected by the mouse to the t-maze environment :\n",
    "    process_layer.inputs.u = link(model_layer,lambda x : x.u)\n",
    "    \n",
    "    return layer_network([process_layer,model_layer],\"t-maze_network\")\n",
    "\n",
    "\n",
    "# 3. The jax results. \n",
    "# in the jax implementation, planning, action selection and learning options\n",
    "# are stored in dictionnaries. \n",
    "from actynf.jax_methods.layer_options import get_planning_options,get_action_selection_options,get_learning_options\n",
    "from actynf.jax_methods.layer_training import synthetic_training\n",
    "\n",
    "def get_tmaze_net_jaxified(Ntrials):\n",
    "    # Training options :\n",
    "    Sh = 2                      # State horizon (or the number of inidividual states that will create their own branch)\n",
    "    remainder_state_bool = True # Do we create an additional branch with the remaining potential state density ?\n",
    "    Ph = 4                      # Policy horizon (or the number of individual actions that will be explored at each node)\n",
    "    option_a_nov = False\n",
    "    option_b_nov = False\n",
    "    additional_options_planning = False    \n",
    "\n",
    "    planning_options = get_planning_options(Th,\"sophisticated\",\n",
    "                            Sh,Ph,remainder_state_bool,\n",
    "                            option_a_nov,option_b_nov,additional_options_planning)\n",
    "\n",
    "    as_options  =get_action_selection_options(\"stochastic\",alpha=16)\n",
    "\n",
    "    learn_options = get_learning_options(True,False,True,run_smoother=True)\n",
    "\n",
    "    training_parrallel_func = partial(synthetic_training,\n",
    "        Ntrials=Ntrials,T=T,\n",
    "        A=true_A,B=true_B,D=true_D,U=U,\n",
    "        a0=true_a,b0=true_b,c=true_c,d0=true_d,e=true_e,\n",
    "        planning_options=planning_options,\n",
    "        action_selection_options = as_options,\n",
    "        learning_options = learn_options)\n",
    "    \n",
    "    return training_parrallel_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Network [t-maze_network] : Timestep 3 / 3\n",
      " Done !   -------- (seeds : [3470-0;9143-0])\n",
      "-------------------------------------------------\n",
      "EFE computed by the classical(numpy) actynf implementation : \n",
      "[[  -11.44075064   -12.86416348   -12.86416348    -9.50241543]\n",
      " [   -6.51113554    -4.63113554 -1009.00484118    -6.51113554]]\n",
      "It took 0.011 seconds.\n",
      "\n",
      "###\n",
      "\n",
      "EFE computed by the jax actynf implementation : \n",
      "[[[-11.44075   -12.918507  -12.918507   -9.51523  ]\n",
      "  [ -6.5111356  -4.631136  -10.391135   -6.5111356]]]\n",
      "It took 2.476 seconds.\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "# Classic version computation time ! \n",
    "t0_classic = time.time()\n",
    "tmaze_net_classic = get_tmaze_net_classic()\n",
    "stm,weights = tmaze_net_classic.run()\n",
    "computed_efe_classic = np.sum(stm[1].Gd,axis=1)\n",
    "delta_t_classic = time.time() - t0_classic\n",
    "\n",
    "\n",
    "# Jax version computation time ! \n",
    "rngkey_training = jr.PRNGKey(300)  # Jax requires a PRNG key before generating pseudo random numbers\n",
    "                                # This will be used in agent action selection and process outcome generation\n",
    "\n",
    "t0_jax = time.time()\n",
    "tmaze_net_jax = get_tmaze_net_jaxified(1) # For a single trial for now !\n",
    "# Let's ensure the function is compiled !\n",
    "tmaze_net_jax_jitted = jax.jit(tmaze_net_jax)\n",
    "[all_obs_arr,all_true_s_arr,all_u_arr,\n",
    "    all_qs_arr,all_qs_post,all_qpi_arr,efes_arr,\n",
    "    a_hist,b_hist,d_hist] = tmaze_net_jax_jitted(rngkey_training)\n",
    "computed_efe_jax = efes_arr.block_until_ready()\n",
    "delta_t_jax = (time.time() - t0_jax)\n",
    "\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"EFE computed by the classical(numpy) actynf implementation : \")\n",
    "print(computed_efe_classic.T)\n",
    "print(f\"It took {delta_t_classic:.3f} seconds.\")\n",
    "print(\"\\n###\\n\")\n",
    "print(\"EFE computed by the jax actynf implementation : \")\n",
    "print(computed_efe_jax)\n",
    "print(f\"It took {delta_t_jax:.3f} seconds.\")\n",
    "print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results match pretty closely ! Our planning algorithms also match the results of SPM's Sophisticated Inference method ! Huzzah :D !\n",
    "\n",
    "However ... it took a while to process this very basic example ! Is it normal ? Isn't jax supposed to allow faster processing than classical numpy ?\n",
    "\n",
    "[The answer isn't that clear cut](https://jax.readthedocs.io/en/latest/faq.html#is-jax-faster-than-numpy), but the most significant discrepancy can be explained by the compilation of the code, as illustrated below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>  With just in time compilatrion (JIT) <<<<<<<<<<<<<<\n",
      "--- First computation ---\n",
      "It took 2.668 seconds.\n",
      "--- Subsequent computations ---\n",
      "It took on average 1.7435 ms (± 0.44742 ms). (1000 samples)\n",
      ">>>>>>>>>>>>  Without just in time compilatrion (JIT) <<<<<<<<<<<<<<\n",
      "--- First computation ---\n",
      "It took 2.214 seconds.\n",
      "--- Subsequent computations ---\n",
      "It took on average 2093.0567 ms (± 98.25488 ms). (100 samples)\n"
     ]
    }
   ],
   "source": [
    "Ntrials = 10\n",
    "tmaze_net_jax_jitted_new = jax.jit(get_tmaze_net_jaxified(Ntrials))  # Get a new function to avoid using a previously compiled function\n",
    "\n",
    "print(\">>>>>>>>>>>>  With just in time compilatrion (JIT) <<<<<<<<<<<<<<\")\n",
    "print(\"--- First computation ---\")\n",
    "t0_jax = time.time()\n",
    "key = rngkey_training\n",
    "efe_value  = tmaze_net_jax_jitted_new(rngkey_training)[6].block_until_ready()\n",
    "delta_t_jax_run1 = (time.time() - t0_jax)\n",
    "print(f\"It took {delta_t_jax_run1:.3f} seconds.\")\n",
    "\n",
    "print(\"--- Subsequent computations ---\")\n",
    "infer_times = []\n",
    "Nsamples = 1000\n",
    "for t in range(Nsamples):\n",
    "    t0_jax = time.time()\n",
    "    rngkey_training = jr.PRNGKey(np.random.randint(0,100))  # Jax requires a PRNG key before generating pseudo random numbers\n",
    "                                # This will be used in agent action selection and process outcome generation\n",
    "    [all_obs_arr,all_true_s_arr,all_u_arr,\n",
    "    all_qs_arr,all_qs_post,all_qpi_arr,efes_arr,\n",
    "    a_hist,b_hist,d_hist] = tmaze_net_jax_jitted_new(rngkey_training)\n",
    "    computed_efe_jax = efes_arr.block_until_ready()\n",
    "    \n",
    "    infer_times.append(time.time() - t0_jax)\n",
    "\n",
    "avg_deltat = 1e3*np.mean(infer_times)\n",
    "std_deltat = 1e3*np.std(infer_times)\n",
    "print(f\"It took on average {avg_deltat:.4f} ms (\\u00B1 {std_deltat:.5f} ms). ({Nsamples} samples)\")\n",
    "\n",
    "\n",
    "print(\">>>>>>>>>>>>  Without just in time compilatrion (JIT) <<<<<<<<<<<<<<\")\n",
    "print(\"--- First computation ---\")\n",
    "tmaze_nojit = get_tmaze_net_jaxified(1)\n",
    "t0_jax = time.time()\n",
    "key = rngkey_training\n",
    "efe_value  = tmaze_nojit(rngkey_training)[6].block_until_ready()\n",
    "delta_t_jax_run1 = (time.time() - t0_jax)\n",
    "print(f\"It took {delta_t_jax_run1:.3f} seconds.\")\n",
    "print(\"--- Subsequent computations ---\")\n",
    "infer_times = []\n",
    "Nsamples = 100\n",
    "for t in range(Nsamples):\n",
    "    t0_jax = time.time()\n",
    "    rngkey_training = jr.PRNGKey(np.random.randint(0,100))  # Jax requires a PRNG key before generating pseudo random numbers\n",
    "                                # This will be used in agent action selection and process outcome generation\n",
    "    [all_obs_arr,all_true_s_arr,all_u_arr,\n",
    "    all_qs_arr,all_qs_post,all_qpi_arr,efes_arr,\n",
    "    a_hist,b_hist,d_hist] = tmaze_nojit(rngkey_training)\n",
    "    computed_efe_jax = efes_arr.block_until_ready()\n",
    "    \n",
    "    infer_times.append(time.time() - t0_jax)\n",
    "\n",
    "avg_deltat = 1e3*np.mean(infer_times)\n",
    "std_deltat = 1e3*np.std(infer_times)\n",
    "print(f\"It took on average {avg_deltat:.4f} ms (\\u00B1 {std_deltat:.5f} ms). ({Nsamples} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtime for the already compiled function seems much more interesting ! The lesson here is to avoid recompilating too often when using the jaxified version ;) !\n",
    "\n",
    "And as you can see, jitting the functions play a huge part in allowing faster computations. Running multiple agents in parrallel is now much more feasible and this allows us to simulate multiple inference and learning schemes using SI !\n",
    "\n",
    "### What's missing\n",
    "\n",
    "At some point, we should compare training-scale results (i.e. how the agents learn across multiple trials). There are some differences in implementation here so the results may not be that close.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Going further \n",
    "\n",
    "We can use this new implementation as well as the auto-differentiable capabilities of the Jax framework to work towards more extensive parameter estimation scheme ! In [the following notebook](jax_Tmaze_inversion.ipynb), we perform model inversion on the T-maze model !\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
