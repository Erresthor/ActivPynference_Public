# -*- coding: utf-8 -*-
"""
Created on Tue May 18 10:55:21 2021

@author: cjsan
"""
import numpy as np
from spm12_implementation import MDP 
import random
import sys
     
# SET UP MODEL STRUCTURE ------------------------------------------------------
#print("-------------------------------------------------------------")
#print("------------------SETTING UP MODEL STRUCTURE-----------------")
#print("-------------------------------------------------------------")
#
def explore_exploit_model(pinit,pHA=1,pWin=1,la=1,rs=3):
    print("Explore-Exploit --- Model set-up ...  ",end='')
    #Points within a trial
    T = 3
    Ni = 16
    
    
    # Priors about initial states
    # Prior probabilities about initial states in the generative process
    D_ =[]
    # Context state factor
    D_.append(np.array([pinit,1-pinit])) #[Left better, right better]
    # Behaviour state factor
    D_.append(np.array([1,0,0,0])) #{'start','hint','choose-left','choose-right'}
    
    # Prior beliefs about initial states in the generative process
    d_ =[]
    # Context beliefs
    d_.append(np.array([0.25,0.25])) #[Left better, right better]
    # Behaviour beliefs
    d_.append(np.array([1,0,0,0])) #{'start','hint','choose-left','choose-right'}
    
    
    # State Outcome mapping and beliefs
    # Prior probabilities about initial states in the generative process
    Ns = [D_[0].shape[0],D_[1].shape[0]] #(Number of states)
    A_ = []
    #Mapping from states to observed hints, accross behaviour states (non represented)
    #
    # [ .  . ]  No hint
    # [ .  . ]  Machine Left Hint            Rows = observations
    # [ .  . ]  Machine Right Hint
    # Left Right
    # Columns = context state
    A_obs_hints = np.zeros((3,Ns[0],Ns[1]))
    A_obs_hints[0,:,:] = 1
    A_obs_hints[:,:,1] = np.array([[0,0],
                             [pHA, 1-pHA],
                             [1-pHA,pHA]]) # Behaviour ste "hint" gives an observed hint
    #Mapping from states to outcome (win / loss / null), accross behaviour states (non represented)
    #
    # [ .  . ]  Null
    # [ .  . ]  Win           Rows = observations
    # [ .  . ]  Loss
    #
    # Columns = context state
    A_obs_outcome = np.zeros((3,Ns[0],Ns[1]))
    A_obs_outcome[0,:,0:2] = 1
    A_obs_outcome[:,:,2] = np.array([[0,0],   # If we choose left, what is the probability of achieving win / loss 
                             [pWin, 1-pWin],
                             [1-pWin,pWin]]) # Choice gives an observable outcome
                   # If true = left, right
    A_obs_outcome[:,:,3] = np.array([[0,0],     # If we choose right, what is the probability of achieving win / loss 
                             [1-pWin, pWin],
                             [pWin,1-pWin]]) # Choice gives an observable outcome
                  # If true = left, right
    #Mapping from behaviour states to observed behaviour
    #
    # [ .  .  .  .] start
    # [ .  .  .  .] hint
    # [ .  .  .  .] choose left         Row = Behaviour state
    # [ .  .  .  .] choose right
    #  s   h  l  r
    #
    # 3rd dimension = observed behaviour
    # The 2nd dimension maps the dependance on context state
    A_obs_behaviour = np.zeros((Ns[1],Ns[0],Ns[1]))
    for i in range (Ns[1]) :
        A_obs_behaviour[i,:,i] = np.array([1,1])
    A_ = [A_obs_hints,A_obs_outcome,A_obs_behaviour]
    a_ = []
    for mod in range (len(A_)):
        a_.append(np.copy(A_[mod])*200)
    
    a_[0][:,:,1] = np.array([[0,0],
                            [0.25,0.25],
                            [0.25,0.25]])
    
    # Transition matrixes between hidden states ( = control states)
    B_ = []
    #a. Transition between context states --> The agent cannot act so there is only one :
    B_context_states = np.array([[[1],[0]],
                                 [[0],[1]]])
    B_.append(B_context_states)
    #b. Transition between behavioural states --> 4 actions
    B_behav_states = np.zeros((Ns[1],Ns[1],Ns[1]))
    # - 0 --> Move to start from any state
    B_behav_states[0,:,0] = 1
    # - 1 --> Move to hint from any state
    B_behav_states[1,:,1] = 1
    # - 2 --> Move to choose left from any state
    B_behav_states[2,:,2] = 1
    # - 3 --> Move to choose right from any state
    B_behav_states[3,:,3] = 1
    B_.append(B_behav_states)
    
    b_ = []
    for fac in range (len(B_)):
        b_.append(np.copy(B_[fac])*200)
    
    b_[0] = np.array([[[0.25],[0.25]],
                           [[0.25],[0.25]]])
    
    
    # Preferred outcomes
    # One matrix per outcome modality. Each row is an observation, and each
    # columns is a time point. Negative values indicate lower preference,
    # positive values indicate a high preference. Stronger preferences promote
    # risky choices and reduced information-seeking.
    No = [A_[0].shape[0],A_[1].shape[0],A_[2].shape[0]]
    
    
    C_hints = np.zeros((No[0],T))
    C_win_loss = np.zeros((No[1],T))
    C_win_loss = np.array([[0,0,0],     #null
                           [0,rs,rs/2],  #win
                           [0,-la,-la]]) #loss
    C_observed_behaviour = np.zeros((No[2],T))
    C_ = [C_hints,C_win_loss,C_observed_behaviour]
    
    
    # Policies
    Np = 5 #Number of policies
    Nf = 2 #Number of state factors
    V_ = np.zeros((T-1,Np,Nf))
    V_[:,:,0]= np.array([[0,0,0,0,0],      # T = 2
                         [0,0,0,0,0]])     # T = 3  row = time point
        #                colums = possible course of action in this modality (0 -->context states)
    V_[:,:,1] = np.array([[0,1,1,2,3],      # T = 2
                         [0,2,3,0,0]])     # T = 3  row = time point in this modality (1 -->behavioural states)
        #                colums = possible course of action
    V_ = V_.astype(np.int)
    
    #Habits
    E_ = None
    e_ = np.ones((Np,))
    
    
    
    
    
    
    model = MDP()
    
    model.T = T
    model.Ni = Ni
    model.A_ = A_
    #model.a_ = a_
    
    model.D_ = D_
    model.d_ = d_
    
    model.B_  = B_
#    model.b_ = b_
    
    model.C_ = C_
    
    model.V_ = V_
    
    #model.E_ = E_
    #model.e_ = e_
    
    #Other parameters
    model.eta = 1 #Learning rate
    model.beta = 1 # expected precision in EFE(pi), higher beta --> lower expected precision
             # low beta --> high influence of habits, less deterministic policiy selection
    model.alpha = 32 # Inverse temperature / Action precision
                # How much randomness in selecting actions
                # (high alpha --> more deterministic)
    model.erp = 4  # degree of belief resetter at each time point
             # ( 1 means no reset bet. time points, higher values mean more loss in confidence)
    model.tau = 4 #Time constant for evidence accumulation
                # magnitude of updates at each iteration of gradient descent
                # high tau --> smaller updates, smaller convergence, greater  stability
    print("Done")
    return model

if (__name__ == "__main__"):
    mod = explore_exploit_model(0.8)
    mod.run()
    print(mod.F)
    print(mod.G)
    print(mod.o)
    print(mod.s)

