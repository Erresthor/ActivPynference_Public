{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> active-pynference </b> : Sophisticated Inference with Jax !\n",
    "\n",
    "<b>actynf</b> implements Sophisticated Inference with jax functions ! This allows us to beneficiate from the Just-In-Time compilation, auto-vectorization and auto-differntiation abilities of the package. This notebook is used to compare the results of the [SPM12's implementation of sophisticated inference](https://github.com/spm/spm/blob/main/toolbox/DEM/spm_MDP_VB_XX.m), the *numpy* implementation of this package, and the *jax* implementation of this package.\n",
    "\n",
    "**Note :** Writing in Jax comes with a number of constraints that don't exist in classical Python (see [this page](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) for the common pitfalls). This means that jax_actynf does not do exactly the same operations as classical sophisticated inference implementations (tree pruning, dynamic variable-dependent conditionning, etc.). Depending on your goal, it may be more interesting to switch to a Jax-based model , or remain in a classical (numpy) based environment. We give a few details regarding this point by the end of this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 . Environment : simple T-maze\n",
    "\n",
    "We'll be using a close analog to the T-maze environment here. The [basics about the T-maze environment](T-maze_demo.ipynb) remain the same, but clue and reward modalities are fused together. The MDP weights for this situation are available in [this](../actynf/demo_tools/tmaze/weights.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actynf version : 0.1.34\n"
     ]
    }
   ],
   "source": [
    "import actynf\n",
    "print(\"Actynf version : \" + str(actynf.__version__))\n",
    "\n",
    "from actynf.demo_tools.tmaze.weights import get_T_maze_gen_process,get_T_maze_model,get_jax_T_maze_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the other demo, the weights depend on scalar parameters describing the properties of the true environment as well as the initial model of the artificial mouse : \n",
    "\n",
    "For the environment (process) :\n",
    "- $p_{init}$ is the probability of the reward being on the right at the beginning.\n",
    "- $p_{HA}$ is the probability of the clue showing the right (resp. left) when the reward is on the right (resp.left).\n",
    "- $p_{win}$ is the probability of getting a positive (resp. adversive) stimulus when picking the reward (resp. the shock).\n",
    "\n",
    "For the mouse model : \n",
    "- $p_{HA}$ is the mouse belief about the mapping of the clue\n",
    "- *initial_hint_confidence* is the strenght of this belief\n",
    "- $la$,$rs$ are the agent priors about receiving adversive vs positive stimuli.\n",
    "- $p_{win}$ is the mouse belief about probability of getting a positive (resp. negative) stimulus when picking the reward (resp. the shock).\n",
    "- *context_belief* is where the mouse thinks the reward spawns at the beginning of each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_process_pHA = 1.0\n",
    "true_process_pinit = 1.0\n",
    "true_process_pwin = 0.98  # For a bit of noise !\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
